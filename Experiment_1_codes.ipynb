{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91861532",
   "metadata": {},
   "source": [
    "# Dataset part again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88eca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and processing \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "whole_df = pd.read_csv(\"C:/Users/Sharafat/Desktop/Masters' Dissertation/Structured_whole_dataframe.csv\")\n",
    "whole_df = whole_df.drop(\"Unnamed: 0\", axis = 1)\n",
    "train_df = whole_df[whole_df[\"source_x\"].astype(str).str.startswith(\"DDICorpus/Train/\")]\n",
    "train_df\n",
    "train_df[\"type\"] = train_df[\"type\"].replace(\"No interaction\", \"no interaction\")\n",
    "train_df\n",
    "test_df = whole_df[whole_df[\"source_x\"].astype(str).str.startswith(\"DDICorpus/Test/\")]\n",
    "test_df\n",
    "test_df[\"type\"] = test_df[\"type\"].replace(\"No interaction\", \"no interaction\")\n",
    "test_df\n",
    "# example => train_df or test_df \n",
    "train_df_records = train_df.to_dict(orient = \"records\")\n",
    "train_df_records\n",
    "test_df_records = test_df.to_dict(orient = \"records\")\n",
    "test_df_records\n",
    "# 1. Sample 10 examples of 'no interaction'\n",
    "no_interaction_df = train_df[train_df['type'] == 'no interaction'].sample(n=10, random_state=42)\n",
    "\n",
    "# 2. Sample 3 examples from each of the *other* types (mechanism, effect, etc.)\n",
    "other_types_df = (\n",
    "    train_df[train_df['type'] != 'no interaction']\n",
    "    .groupby('type', group_keys=False)\n",
    "    .sample(n=3, random_state=42)\n",
    ")\n",
    "\n",
    "# 3. Combine and shuffle\n",
    "examples_df = pd.concat([no_interaction_df, other_types_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "examples_df[\"type\"] = examples_df[\"type\"].replace(\"No interaction\", \"no interaction\")\n",
    "examples_df\n",
    "example_shots = examples_df.to_dict(orient=\"records\")\n",
    "example_shots\n",
    "train_df_grouped = train_df.groupby(\"sentence_id\")\n",
    "train_df_grouped\n",
    "group = train_df_grouped.get_group(\"DDI-DrugBank.d64.s87\")\n",
    "group\n",
    "train_df[\"triplet\"] = list(zip(train_df[\"e1_text\"], train_df[\"type\"], train_df[\"e2_text\"]))\n",
    "train_df\n",
    "import json\n",
    "\n",
    "# Group by sentence (assuming sentence text is unique identifier)\n",
    "grouped = train_df.groupby(\"sentence_text\")\n",
    "\n",
    "# Build JSON-like structure\n",
    "json_data = []\n",
    "\n",
    "for sentence_text, group in grouped:\n",
    "    # Get triplets from the group\n",
    "    triplets = list(zip(group[\"e1_text\"], group[\"type\"], group[\"e2_text\"]))\n",
    "    \n",
    "    # Optional: if 'type4' column exists, get the most frequent or first type4\n",
    "    type4_value = group[\"type\"].iloc[0]\n",
    "    \n",
    "    # Create record\n",
    "    json_data.append({\n",
    "        \"sentence\": sentence_text,\n",
    "        \"interaction_type\": type4_value,\n",
    "        \"triplets\": triplets\n",
    "    })\n",
    "\n",
    "# Save to file\n",
    "with open(\"train_df_sentence_triplets.json\", \"w\") as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "\n",
    "with open(\"train_df_sentence_triplets.json\",\"r\") as f:\n",
    "    train_triplets = json.load(f)\n",
    "\n",
    "train_triplets[0]\n",
    "import json\n",
    "\n",
    "# Group by sentence (assuming sentence text is unique identifier)\n",
    "grouped = test_df.groupby(\"sentence_text\")\n",
    "\n",
    "# Build JSON-like structure\n",
    "json_data = []\n",
    "\n",
    "for sentence_text, group in grouped:\n",
    "    # Get triplets from the group\n",
    "    triplets = list(zip(group[\"e1_text\"], group[\"type\"], group[\"e2_text\"]))\n",
    "    \n",
    "    # Optional: if 'type4' column exists, get the most frequent or first type4\n",
    "    type4_value = group[\"type\"].iloc[0]\n",
    "    \n",
    "    # Create record\n",
    "    json_data.append({\n",
    "        \"sentence\": sentence_text,\n",
    "        \"interaction_type\": type4_value,\n",
    "        \"triplets\": triplets\n",
    "    })\n",
    "\n",
    "# Save to file\n",
    "with open(\"test_df_sentence_triplets.json\", \"w\") as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "with open(\"test_df_sentence_triplets.json\",\"r\") as f:\n",
    "    test_triplets = json.load(f)\n",
    "\n",
    "test_triplets[0]\n",
    "import json\n",
    "\n",
    "# Group by unique sentence (or sentence_id if preferred)\n",
    "grouped = train_df.groupby(\"sentence_text\")\n",
    "\n",
    "# Build JSON-like structure\n",
    "sentence_entities = []\n",
    "\n",
    "for sentence_text, group in grouped:\n",
    "    # Collect unique entities from both drug1 and drug2\n",
    "    entities = set(group[\"e1_text\"]).union(set(group[\"e2_text\"]))\n",
    "    \n",
    "    sentence_entities.append({\n",
    "        \"sentence\": sentence_text,\n",
    "        \"entities\": sorted(entities)  # optional: sort for consistency\n",
    "    })\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"train_df_sentence_entities.json\", \"w\") as f:\n",
    "    json.dump(sentence_entities, f, indent=2)\n",
    "\n",
    "with open(\"train_df_sentence_entities.json\",\"r\") as f:\n",
    "    train_entities = json.load(f)\n",
    "\n",
    "train_entities[0]\n",
    "train_triplets\n",
    "few_shot_examples = train_triplets\n",
    "few_shot_examples\n",
    "import json\n",
    "\n",
    "# Group by unique sentence (or sentence_id if preferred)\n",
    "grouped = test_df.groupby(\"sentence_text\")\n",
    "\n",
    "# Build JSON-like structure\n",
    "sentence_entities = []\n",
    "\n",
    "for sentence_text, group in grouped:\n",
    "    # Collect unique entities from both drug1 and drug2\n",
    "    entities = set(group[\"e1_text\"]).union(set(group[\"e2_text\"]))\n",
    "    \n",
    "    sentence_entities.append({\n",
    "        \"sentence\": sentence_text,\n",
    "        \"entities\": sorted(entities)  # optional: sort for consistency\n",
    "    })\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"test_df_sentence_entities.json\", \"w\") as f:\n",
    "    json.dump(sentence_entities, f, indent=2)\n",
    "\n",
    "with open(\"test_df_sentence_entities.json\",\"r\") as f:\n",
    "    test_entities = json.load(f)\n",
    "\n",
    "test_entities[0]\n",
    "len(train_entities)\n",
    "len(test_entities)\n",
    "examples_df\n",
    "import json\n",
    "\n",
    "# Group by sentence (assuming sentence text is unique identifier)\n",
    "grouped = examples_df.groupby(\"sentence_text\")\n",
    "\n",
    "# Build JSON-like structure\n",
    "json_data = []\n",
    "\n",
    "for sentence_text, group in grouped:\n",
    "    # Get triplets from the group\n",
    "    triplets = list(zip(group[\"e1_text\"], group[\"type\"], group[\"e2_text\"]))\n",
    "    \n",
    "    # Optional: if 'type4' column exists, get the most frequent or first type4\n",
    "    type4_value = group[\"type\"].iloc[0]\n",
    "    \n",
    "    # Create record\n",
    "    json_data.append({\n",
    "        \"sentence\": sentence_text,\n",
    "        \"interaction_type\": type4_value,\n",
    "        \"triplets\": triplets\n",
    "    })\n",
    "\n",
    "# Save to file\n",
    "with open(\"few_shot_example_triplets.json\", \"w\") as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "\n",
    "with open(\"few_shot_example_triplets.json\",\"r\") as f:\n",
    "    few_shot_entities = json.load(f)\n",
    "\n",
    "few_shot_entities[0]\n",
    "len(few_shot_entities)\n",
    "train_entities\n",
    "train_triplets # ground truth, remove interaction_type and choose random examples for few_shot\n",
    "def transform_structure(data):\n",
    "    output = {'sentence': data['sentence']}\n",
    "    for idx, entity in enumerate(data['entities'], 1):\n",
    "        output[f'Entity_{idx}'] = entity\n",
    "    return output\n",
    "\n",
    "transformed = transform_structure(train_entities[0])\n",
    "print(transformed)\n",
    "\n",
    "def transform_structure(data):\n",
    "    output = {'Sentence': data['sentence']}\n",
    "    for idx, entity in enumerate(data['entities'], 1):\n",
    "        output[f'Entity_{idx}'] = entity\n",
    "    return output\n",
    "\n",
    "updated_train_entities = []\n",
    "for i in range(len(train_entities)):\n",
    "    transformed = transform_structure(train_entities[i])\n",
    "    updated_train_entities.append(transformed)\n",
    "print(updated_train_entities[-1])\n",
    "len(updated_train_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4bb1d0",
   "metadata": {},
   "source": [
    "# Sentence Level Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c46cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "GEMINI_API_KEY = ''\n",
    "assert GEMINI_API_KEY, \"\"\n",
    "\n",
    "# Set your Gemini API key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
    "\n",
    "\n",
    "\n",
    "from google import genai\n",
    "\n",
    "client = genai.Client()\n",
    "    \n",
    "llm_zero_shot_responses = []\n",
    "for i in range(len(updated_train_entities)):  # updated_train_entities file is given in both github and zip file\n",
    "  query = f\"\"\"You are a biomedical expert. Such sentence and all drug names in the sentence are given: {updated_train_entities[i]}.\n",
    "\n",
    "Interaction types are: \n",
    "\n",
    "\"mechanism\": \"Biological process causing interaction (e.g., enzyme inhibition)\",\n",
    "    \"effect\": \"Clinical outcome (e.g., increased drug concentration)\",\n",
    "    \"advise\": \"Recommendation (e.g., avoid concomitant use)\",\n",
    "    \"int\": \"General interaction mention without specifics\",\n",
    "    \"no interaction\": \"No explicit or implicit sign of interaction\"\n",
    "\n",
    "Find the interaction type between all possible pairs of these entities and extract triplets in such format: (entity_1, interaction_type, entity_2)\n",
    "\n",
    "Answer with only triplets\n",
    "Triplets:  \"\"\"\n",
    "  response = client.models.generate_content(\n",
    "  model=\"gemma-3-27b-it\",\n",
    "  contents=query,\n",
    "  config=genai.types.GenerateContentConfig(\n",
    "      temperature = 0\n",
    "    \n",
    "  )\n",
    ")\n",
    "  time.sleep(3.5)\n",
    "  llm_zero_shot_responses.append((updated_train_entities[i], response.text))\n",
    "\n",
    "  with open(\"llm_triplet_zero_shot_responses.json\",\"w\") as f:\n",
    "    json.dump(f\"Result_{i}: updated_train_entities[i], response.text\", f , indent = 2)\n",
    "  print(f\"Predicting {i}\")\n",
    "  print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068c5248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Your model predictions: list of (sentence_dict, prediction_text)\n",
    "\n",
    "# Convert prediction text into structured triplets\n",
    "def parse_triplets(text):\n",
    "    triplet_lines = text.strip().split(\"\\n\")\n",
    "    triplets = []\n",
    "    for line in triplet_lines:\n",
    "        if line.startswith(\"(\") and line.endswith(\")\"):\n",
    "            try:\n",
    "                parts = line.strip(\"()\").split(\", \")\n",
    "                if len(parts) == 3:\n",
    "                    triplets.append([parts[0], parts[1], parts[2]])\n",
    "            except Exception:\n",
    "                continue\n",
    "    return triplets\n",
    "\n",
    "# Merge predictions with ground truth\n",
    "merged_results = []\n",
    "for pred in llm_zero_shot_responses:\n",
    "    sentence_data, prediction_text = pred\n",
    "    sentence = sentence_data['Sentence']\n",
    "    predicted_triplets = parse_triplets(prediction_text)\n",
    "\n",
    "    # Find matching ground truth entry\n",
    "    gt_entry = next((item for item in train_triplets if item['sentence'].strip() == sentence.strip()), None)\n",
    "    ground_truth_triplets = gt_entry['triplets'] if gt_entry else []\n",
    "\n",
    "    merged_results.append({\n",
    "        \"sentence\": sentence,\n",
    "        \"ground_truth_triplets\": ground_truth_triplets,\n",
    "        \"predicted_triplets\": predicted_triplets\n",
    "    })\n",
    "\n",
    "# Save merged results\n",
    "with open(\"merged_predictions_vs_ground_truth.json\", \"w\") as f:\n",
    "    json.dump(merged_results, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c4e1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def create_few_shot_prompt(train_triplets, k=5, seed=42):\n",
    "    random.seed(seed)\n",
    "    selected = random.sample(train_triplets, k)\n",
    "\n",
    "    examples = []\n",
    "    for ex in selected:\n",
    "        sentence = ex[\"sentence\"].strip()\n",
    "        triplets = ex.get(\"triplets\", [])\n",
    "\n",
    "        # Get unique entities from the triplets\n",
    "        entity_set = set()\n",
    "        for t in triplets:\n",
    "            entity_set.add(t[0])\n",
    "            entity_set.add(t[2])\n",
    "        entities = \", \".join(sorted(entity_set))\n",
    "\n",
    "        triplet_lines = \"\\n\".join([f\"({t[0]}, {t[1]}, {t[2]})\" for t in triplets])\n",
    "\n",
    "        examples.append(\n",
    "            f\"\"\"Example:\n",
    "Sentence: \"{sentence}\"\n",
    "Entities: {entities}\n",
    "Triplets:\n",
    "{triplet_lines}\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(examples)\n",
    "\n",
    "few_shot_prompt_prefix = create_few_shot_prompt(train_triplets, k=5)\n",
    "\n",
    "# Then use in your prompting like:\n",
    "few_shot_prompt = f\"{few_shot_prompt_prefix}\"\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b24b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from google import genai\n",
    "\n",
    "\n",
    "GEMINI_API_KEY = '' # it is sensitive informative, that is why I removed it\n",
    "assert GEMINI_API_KEY, \"\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
    "client = genai.Client()\n",
    "\n",
    "# Output holder\n",
    "all_responses = []\n",
    "\n",
    "# Prompt styles\n",
    "interaction_types = \"\"\"Interaction types are:\n",
    "\"mechanism\": \"Biological process causing interaction (e.g., enzyme inhibition)\",\n",
    "\"effect\": \"Clinical outcome (e.g., increased drug concentration)\",\n",
    "\"advise\": \"Recommendation (e.g., avoid concomitant use)\",\n",
    "\"int\": \"General interaction mention without specifics\",\n",
    "\"no interaction\": \"No explicit or implicit sign of interaction\"\n",
    "\"\"\"\n",
    "\n",
    "few_shot_examples = f\"\"\"\n",
    "{interaction_types}\n",
    "\"\"\"\n",
    "\n",
    "cot_instruction = f\"\"\"\n",
    "Let's think step by step. First identify all unique entity pairs. Then, analyze the sentence for any interaction clues using the definitions below. Finally, write each result in the format: (entity_1, interaction_type, entity_2)\n",
    "\n",
    "{interaction_types}\n",
    "\"\"\"\n",
    "\n",
    "def extract_entities(entity_dict):\n",
    "    return [v for k, v in entity_dict.items() if k.startswith(\"Entity_\")]\n",
    "\n",
    "# Inference loop\n",
    "for i, example in enumerate(updated_train_entities):\n",
    "    sentence = example[\"Sentence\"]\n",
    "    entities = extract_entities(example)\n",
    "\n",
    "    output_entry = {\n",
    "        \"index\": i,\n",
    "        \"sentence\": sentence,\n",
    "        \"entities\": entities\n",
    "    }\n",
    "\n",
    "    # Prepare prompts\n",
    "    zero_shot_prompt = f\"\"\"You are a biomedical expert.\n",
    "\n",
    "Sentence: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "{interaction_types}\n",
    "\n",
    "Find the interaction type between all possible pairs of these entities and extract triplets in this format:\n",
    "(entity_1, interaction_type, entity_2)\n",
    "\n",
    "Answer with only triplets. Do not include any additional text.\n",
    "Triplets:\"\"\"\n",
    "\n",
    "    few_shot_prompt = f\"\"\"{few_shot_examples}\n",
    "\n",
    "Sentence: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Answer with only triplets. Do not include any additional text.\n",
    "Triplets:\"\"\"\n",
    "\n",
    "    cot_prompt = f\"\"\"You are a biomedical expert.\n",
    "\n",
    "Sentence: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "{cot_instruction}\n",
    "\n",
    "Answer with only triplets. Do not include any additional text.\n",
    "Triplets:\"\"\"\n",
    "\n",
    "    prompts = {\n",
    "        \"zero_shot_output\": zero_shot_prompt,\n",
    "        \"few_shot_output\": few_shot_prompt,\n",
    "        \"cot_output\": cot_prompt\n",
    "    }\n",
    "\n",
    "    for method_name, prompt in prompts.items():\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemma-3-27b-it\",\n",
    "                contents=prompt,\n",
    "                config=genai.types.GenerateContentConfig(\n",
    "                    temperature=0.0\n",
    "                )\n",
    "            )\n",
    "            time.sleep(3.5)\n",
    "\n",
    "            output_entry[method_name] = response.text.strip()\n",
    "            print(f\"[{method_name}] Finished index {i}\")\n",
    "            print(f\"{method_name}: {response.text.strip()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {i} with method {method_name}: {e}\")\n",
    "            output_entry[method_name] = \"ERROR\"\n",
    "\n",
    "    all_responses.append(output_entry)\n",
    "\n",
    "    # Save after each example (optional safety)\n",
    "    with open(\"triplet_predictions_all_methods.json\", \"w\") as f:\n",
    "        json.dump(all_responses, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee235825",
   "metadata": {},
   "source": [
    "# Pair Level Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40efdfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Create expanded pairwise dataset\n",
    "pairwise_data = []\n",
    "\n",
    "for idx, item in enumerate(train_triplets):\n",
    "    sentence = item['sentence']\n",
    "    triplets = item['triplets']\n",
    "    \n",
    "    # Extract all unique entities from the triplets\n",
    "    all_entities = set()\n",
    "    for t in triplets:\n",
    "        all_entities.add(t[0])\n",
    "        all_entities.add(t[2])\n",
    "    \n",
    "    # Generate all possible entity pairs\n",
    "    entity_pairs = list(itertools.combinations(all_entities, 2))\n",
    "    \n",
    "    # Create ground truth mapping for quick lookup\n",
    "    gt_map = {}\n",
    "    for t in triplets:\n",
    "        key1 = (t[0], t[2])\n",
    "        key2 = (t[2], t[0])\n",
    "        gt_map[key1] = t[1]\n",
    "        gt_map[key2] = t[1]  # Relations are bidirectional\n",
    "    \n",
    "    # Create entries for each pair\n",
    "    for e1, e2 in entity_pairs:\n",
    "        # Determine ground truth\n",
    "        gt = gt_map.get((e1, e2), \"no interaction\")\n",
    "        \n",
    "        # For explicit \"no interaction\" in ground truth\n",
    "        if (e1, e2) in gt_map and gt_map[(e1, e2)] == \"no interaction\":\n",
    "            gt = \"no interaction\"\n",
    "        \n",
    "        pairwise_data.append({\n",
    "            \"sentence_id\": idx,\n",
    "            \"sentence\": sentence,\n",
    "            \"entity1\": e1,\n",
    "            \"entity2\": e2,\n",
    "            \"ground_truth\": gt\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "pairwise_df = pd.DataFrame(pairwise_data)\n",
    "pairwise_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c09e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_df_records = pairwise_df.to_dict(orient=\"records\")\n",
    "pairwise_df_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e1cd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open(\"pairwise_predictions.json\", \"r\") as f :\n",
    "    all_responses = json.load(f)\n",
    "all_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc80ce4",
   "metadata": {},
   "source": [
    "# Few-shot random examples selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b5924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def create_few_shot_prompt(train_triplets, k=2, seed=42):\n",
    "    random.seed(seed)\n",
    "    selected = random.sample(train_triplets, k)\n",
    "\n",
    "    examples = []\n",
    "    for ex in selected:\n",
    "        sentence = ex[\"sentence\"].strip()\n",
    "        triplets = ex.get(\"triplets\", [])\n",
    "\n",
    "        # Get unique entities from the triplets\n",
    "        entity_set = set()\n",
    "        for t in triplets:\n",
    "            entity_set.add(t[0])\n",
    "            entity_set.add(t[2])\n",
    "        entities = \", \".join(sorted(entity_set))\n",
    "\n",
    "        triplet_lines = \"\\n\".join([f\"({t[0]}, {t[1]}, {t[2]})\" for t in triplets])\n",
    "\n",
    "        examples.append(\n",
    "            f\"\"\"Example:\n",
    "Sentence: \"{sentence}\"\n",
    "Entities: {entities}\n",
    "Triplets:\n",
    "{triplet_lines}\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(examples)\n",
    "\n",
    "few_shot_prompt_2 = create_few_shot_prompt(train_triplets, k=2)\n",
    "\n",
    "# Then use in your prompting like:\n",
    "few_shot_prompt_2_examples = f\"{few_shot_prompt_2}\"\n",
    "print(few_shot_prompt_2_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c923e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def create_few_shot_prompt(train_triplets, k=5, seed=40):\n",
    "    random.seed(seed)\n",
    "    selected = random.sample(train_triplets, k)\n",
    "\n",
    "    examples = []\n",
    "    for ex in selected:\n",
    "        sentence = ex[\"sentence\"].strip()\n",
    "        triplets = ex.get(\"triplets\", [])\n",
    "\n",
    "        # Get unique entities from the triplets\n",
    "        entity_set = set()\n",
    "        for t in triplets:\n",
    "            entity_set.add(t[0])\n",
    "            entity_set.add(t[2])\n",
    "        entities = \", \".join(sorted(entity_set))\n",
    "\n",
    "        triplet_lines = \"\\n\".join([f\"({t[0]}, {t[1]}, {t[2]})\" for t in triplets])\n",
    "\n",
    "        examples.append(\n",
    "            f\"\"\"Example:\n",
    "Sentence: \"{sentence}\"\n",
    "Entities: {entities}\n",
    "Triplets:\n",
    "{triplet_lines}\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(examples)\n",
    "\n",
    "few_shot_prompt_5 = create_few_shot_prompt(train_triplets, k=5)\n",
    "\n",
    "# Then use in your prompting like:\n",
    "few_shot_prompt_5_examples = f\"{few_shot_prompt_5}\"\n",
    "print(few_shot_prompt_5_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5583070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def create_few_shot_prompt(train_triplets, k=10, seed=10):\n",
    "    random.seed(seed)\n",
    "    selected = random.sample(train_triplets, k)\n",
    "\n",
    "    examples = []\n",
    "    for ex in selected:\n",
    "        sentence = ex[\"sentence\"].strip()\n",
    "        triplets = ex.get(\"triplets\", [])\n",
    "\n",
    "        # Get unique entities from the triplets\n",
    "        entity_set = set()\n",
    "        for t in triplets:\n",
    "            entity_set.add(t[0])\n",
    "            entity_set.add(t[2])\n",
    "        entities = \", \".join(sorted(entity_set))\n",
    "\n",
    "        triplet_lines = \"\\n\".join([f\"({t[0]}, {t[1]}, {t[2]})\" for t in triplets])\n",
    "\n",
    "        examples.append(\n",
    "            f\"\"\"Example:\n",
    "Sentence: \"{sentence}\"\n",
    "Entities: {entities}\n",
    "Triplets:\n",
    "{triplet_lines}\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(examples)\n",
    "\n",
    "few_shot_prompt_10 = create_few_shot_prompt(train_triplets, k=10)\n",
    "\n",
    "# Then use in your prompting like:\n",
    "few_shot_prompt_10_examples = f\"{few_shot_prompt_10}\"\n",
    "print(few_shot_prompt_10_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9087b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def create_few_shot_prompt(train_triplets, k=20, seed=20):\n",
    "    random.seed(seed)\n",
    "    selected = random.sample(train_triplets, k)\n",
    "\n",
    "    examples = []\n",
    "    for ex in selected:\n",
    "        sentence = ex[\"sentence\"].strip()\n",
    "        triplets = ex.get(\"triplets\", [])\n",
    "\n",
    "        # Get unique entities from the triplets\n",
    "        entity_set = set()\n",
    "        for t in triplets:\n",
    "            entity_set.add(t[0])\n",
    "            entity_set.add(t[2])\n",
    "        entities = \", \".join(sorted(entity_set))\n",
    "\n",
    "        triplet_lines = \"\\n\".join([f\"({t[0]}, {t[1]}, {t[2]})\" for t in triplets])\n",
    "\n",
    "        examples.append(\n",
    "            f\"\"\"Example:\n",
    "Sentence: \"{sentence}\"\n",
    "Entities: {entities}\n",
    "Triplets:\n",
    "{triplet_lines}\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(examples)\n",
    "\n",
    "few_shot_prompt_20 = create_few_shot_prompt(train_triplets, k=20)\n",
    "\n",
    "# Then use in your prompting like:\n",
    "few_shot_prompt_20_examples = f\"{few_shot_prompt_20}\"\n",
    "print(few_shot_prompt_20_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f97816c",
   "metadata": {},
   "source": [
    "# Pair Level Model with all prompting techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d1ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from google import genai\n",
    "\n",
    "\n",
    "GEMINI_API_KEY = ''\n",
    "assert GEMINI_API_KEY, \"\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
    "client = genai.Client()\n",
    "\n",
    "# Output holder\n",
    "#all_responses = []\n",
    "\n",
    "# Prompt styles\n",
    "interaction_types = \"\"\"\n",
    "- mechanism: Pharmacokinetic interactions affecting drug metabolism\n",
    "- effect: Pharmacodynamic effects or clinical outcomes\n",
    "- advice: Recommendations, cautions, or contraindications\n",
    "- int: General interaction mentions without specifics\n",
    "- no interaction: Explicit statements of no interaction\n",
    "\"\"\"\n",
    "\n",
    "few_shot_examples = f\"\"\"\n",
    "{interaction_types}\n",
    "\"\"\"\n",
    "\n",
    "# Inference loop\n",
    "for i in range(8878, len(pairwise_df_records)):\n",
    "    example = pairwise_df_records[i]\n",
    "    sentence = example[\"sentence\"]\n",
    "    entity1 = example[\"entity1\"]\n",
    "    entity2 = example[\"entity2\"]\n",
    "    ground_truth = example[\"ground_truth\"]\n",
    "\n",
    "    output_entry = {\n",
    "        \"index\": i,\n",
    "        \"sentence\": sentence,\n",
    "        \"entity1\": entity1,\n",
    "        \"entity2\": entity2,\n",
    "        \"ground_truth_label\":ground_truth\n",
    "    }\n",
    "\n",
    "    # Prepare prompts\n",
    "    zero_shot_prompt = f\"\"\"Analyze drug interactions in the text below. Extract EXACTLY ONE label per drug pair using these interaction types:\n",
    "- mechanism: Pharmacokinetic interactions affecting drug metabolism\n",
    "- effect: Pharmacodynamic effects or clinical outcomes\n",
    "- advice: Recommendations, cautions, or contraindications\n",
    "- int: General interaction mentions without specifics\n",
    "- no interaction: Explicit statements of no interaction\n",
    "\n",
    "Rules:\n",
    "1. For cautionary statements like \"Caution is advised\", always use \"advice\"\n",
    "2. Output only one triplet per unique drug pair\n",
    "3. Use normalized entity names from the text\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities are given: {entity1} and {entity2}\n",
    "\n",
    "Answer with only label name. Do not include any additional text.\n",
    "Extract EXACTLY ONE label per drug pair using these interaction types\n",
    "Label:\"\"\"\n",
    "\n",
    "    few_shot_prompt_2 = f\"\"\"Classify drug interactions using these examples:\n",
    "    \n",
    "    {few_shot_prompt_2_examples}\n",
    "\n",
    "Now classify interactions in this new text: \"{sentence}\"\n",
    "Entities are given: {entity1} and {entity2}\n",
    "Output ONLY ONE label name for given pair out of these classes: mechanism, effect, advice, int, no interaction.\n",
    "Label:\"\"\"\n",
    "    \n",
    "    few_shot_prompt_5 = f\"\"\"Classify drug interactions using these examples:\n",
    "    \n",
    "    {few_shot_prompt_5_examples}\n",
    "\n",
    "Now classify interactions in this new text: \"{sentence}\"\n",
    "Entities are given: {entity1} and {entity2}\n",
    "Output ONLY ONE label name for given pair out of these classes: mechanism, effect, advice, int, no interaction.\n",
    "Label:\"\"\"\n",
    "\n",
    "\n",
    "    few_shot_prompt_10 = f\"\"\"Classify drug interactions using these examples:\n",
    "    \n",
    "    {few_shot_prompt_10_examples}\n",
    "\n",
    "Now classify interactions in this new text: \"{sentence}\"\n",
    "Entities are given: {entity1} and {entity2}\n",
    "Output ONLY ONE label name for given pair out of these classes: mechanism, effect, advice, int, no interaction.\n",
    "Label:\"\"\"\n",
    "\n",
    "\n",
    "    few_shot_prompt_20 = f\"\"\"Classify drug interactions using these examples:\n",
    "    \n",
    "    {few_shot_prompt_20_examples}\n",
    "\n",
    "Now classify interactions in this new text: \"{sentence}\"\n",
    "Entities are given: {entity1} and {entity2}\n",
    "Output ONLY ONE label name for given pair out of these classes: mechanism, effect, advice, int, no interaction.\n",
    "Label:\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    cot_prompt = f\"\"\"Analyze drug interactions step by step:\n",
    "\n",
    "1. Identify given drug entities\n",
    "2. For given drug pair:\n",
    "   a. Check if text explicitly says \"no interaction\" → \"no interaction\"\n",
    "   b. Check for caution words: \"caution\", \"advised\", \"warning\" → \"advice\"\n",
    "   c. Check for metabolic terms: \"metabolism\", \"inhibits\", \"induces\" → \"mechanism\"\n",
    "   d. Check for clinical outcomes: \"caused\", \"resulted in\", \"increased\" → \"effect\"\n",
    "   e. Generic \"interaction\" mention → \"int\"\n",
    "3. If multiple signals, choose the strongest:\n",
    "   advice > mechanism > effect > int > no interaction\n",
    "4. Output one triplet: (Entity1, interaction_type, Entity2)\n",
    "\n",
    "Text: \"Caution is advised when beginning, discontinuing, or changing the dose of DIAMOX in patients receiving primidone.\"\n",
    "\n",
    "Step-by-step reasoning:\n",
    "1. Entities: DIAMOX, primidone\n",
    "2. \"Caution is advised\" → advice\n",
    "3. No stronger signals\n",
    "4. Output: advice\n",
    "\n",
    "Now analyze: \"{sentence}\"\n",
    "Now classify interactions in this new text: \"{sentence}\"\n",
    "Entities are given: {entity1} and {entity2}\n",
    "Output ONLY ONE label name for given pair out of these classes: mechanism, effect, advice, int, no interaction.\n",
    "Reasoning Steps:[Gemma's Reasoning]\n",
    "Answer with only label name. Do not include any additional text.\n",
    "Label:\n",
    "\"\"\"\n",
    "\n",
    "    prompts = {\n",
    "        \"zero_shot_output\": zero_shot_prompt,\n",
    "        \"few_shot_output_2\": few_shot_prompt_2,\n",
    "        \"few_shot_output_5\": few_shot_prompt_5,\n",
    "        \"few_shot_output_10\": few_shot_prompt_10,\n",
    "        \"few_shot_output_20\": few_shot_prompt_20,\n",
    "        \"cot_output\": cot_prompt\n",
    "    }\n",
    "\n",
    "    for method_name, prompt in prompts.items():\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemma-3-27b-it\",\n",
    "                contents=prompt,\n",
    "                config=genai.types.GenerateContentConfig(\n",
    "                    temperature=0.0\n",
    "                )\n",
    "            )\n",
    "            time.sleep(3.5)\n",
    "\n",
    "            output_entry[method_name] = response.text.strip()\n",
    "            print(f\"[{method_name}] Finished index {i}\")\n",
    "            print(f\"{method_name}: {response.text.strip()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {i} with method {method_name}: {e}\")\n",
    "            output_entry[method_name] = \"ERROR\"\n",
    "\n",
    "    all_responses.append(output_entry)\n",
    "\n",
    "    # Save after each example (optional safety)\n",
    "    with open(\"pairwise_predictions.json\", \"w\") as f:\n",
    "        json.dump(all_responses, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad82746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5461a237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbea25d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219c9302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6480adb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b466203a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354174df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe22fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e176b832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f12df54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d02250f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac05ef6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51531b94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
