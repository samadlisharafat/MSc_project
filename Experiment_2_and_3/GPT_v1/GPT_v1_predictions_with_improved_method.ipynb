{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f65ee17",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcdcb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and processing \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "whole_df = pd.read_csv(\"C:/Users/Sharafat/Desktop/Masters' Dissertation/Structured_whole_dataframe.csv\")\n",
    "whole_df = whole_df.drop(\"Unnamed: 0\", axis = 1)\n",
    "train_df = whole_df[whole_df[\"source_x\"].astype(str).str.startswith(\"DDICorpus/Train/\")]\n",
    "train_df\n",
    "train_df[\"type\"] = train_df[\"type\"].replace(\"No interaction\", \"no interaction\")\n",
    "train_df\n",
    "test_df = whole_df[whole_df[\"source_x\"].astype(str).str.startswith(\"DDICorpus/Test/\")]\n",
    "test_df\n",
    "test_df[\"type\"] = test_df[\"type\"].replace(\"No interaction\", \"no interaction\")\n",
    "test_df\n",
    "# example => train_df or test_df \n",
    "train_df_records = train_df.to_dict(orient = \"records\")\n",
    "train_df_records\n",
    "test_df_records = test_df.to_dict(orient = \"records\")\n",
    "test_df_records\n",
    "# 1. Sample 10 examples of 'no interaction'\n",
    "no_interaction_df = train_df[train_df['type'] == 'no interaction'].sample(n=10, random_state=42)\n",
    "\n",
    "# 2. Sample 3 examples from each of the *other* types (mechanism, effect, etc.)\n",
    "other_types_df = (\n",
    "    train_df[train_df['type'] != 'no interaction']\n",
    "    .groupby('type', group_keys=False)\n",
    "    .sample(n=3, random_state=42)\n",
    ")\n",
    "\n",
    "# 3. Combine and shuffle\n",
    "examples_df = pd.concat([no_interaction_df, other_types_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "examples_df[\"type\"] = examples_df[\"type\"].replace(\"No interaction\", \"no interaction\")\n",
    "examples_df\n",
    "example_shots = examples_df.to_dict(orient=\"records\")\n",
    "example_shots\n",
    "train_df_grouped = train_df.groupby(\"sentence_id\")\n",
    "train_df_grouped\n",
    "group = train_df_grouped.get_group(\"DDI-DrugBank.d64.s87\")\n",
    "group\n",
    "train_df[\"triplet\"] = list(zip(train_df[\"e1_text\"], train_df[\"type\"], train_df[\"e2_text\"]))\n",
    "train_df\n",
    "import json\n",
    "\n",
    "# Group by sentence (assuming sentence text is unique identifier)\n",
    "grouped = train_df.groupby(\"sentence_text\")\n",
    "\n",
    "# Build JSON-like structure\n",
    "json_data = []\n",
    "\n",
    "for sentence_text, group in grouped:\n",
    "    # Get triplets from the group\n",
    "    triplets = list(zip(group[\"e1_text\"], group[\"type\"], group[\"e2_text\"]))\n",
    "    \n",
    "    # Optional: if 'type4' column exists, get the most frequent or first type4\n",
    "    type4_value = group[\"type\"].iloc[0]\n",
    "    \n",
    "    # Create record\n",
    "    json_data.append({\n",
    "        \"sentence\": sentence_text,\n",
    "        \"interaction_type\": type4_value,\n",
    "        \"triplets\": triplets\n",
    "    })\n",
    "\n",
    "# Save to file\n",
    "with open(\"train_df_sentence_triplets.json\", \"w\") as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "\n",
    "with open(\"train_df_sentence_triplets.json\",\"r\") as f:\n",
    "    train_triplets = json.load(f)\n",
    "\n",
    "train_triplets[0]\n",
    "import json\n",
    "\n",
    "# Group by sentence (assuming sentence text is unique identifier)\n",
    "grouped = test_df.groupby(\"sentence_text\")\n",
    "\n",
    "# Build JSON-like structure\n",
    "json_data = []\n",
    "\n",
    "for sentence_text, group in grouped:\n",
    "    # Get triplets from the group\n",
    "    triplets = list(zip(group[\"e1_text\"], group[\"type\"], group[\"e2_text\"]))\n",
    "    \n",
    "    # Optional: if 'type4' column exists, get the most frequent or first type4\n",
    "    type4_value = group[\"type\"].iloc[0]\n",
    "    \n",
    "    # Create record\n",
    "    json_data.append({\n",
    "        \"sentence\": sentence_text,\n",
    "        \"interaction_type\": type4_value,\n",
    "        \"triplets\": triplets\n",
    "    })\n",
    "\n",
    "# Save to file\n",
    "with open(\"test_df_sentence_triplets.json\", \"w\") as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "with open(\"test_df_sentence_triplets.json\",\"r\") as f:\n",
    "    test_triplets = json.load(f)\n",
    "\n",
    "test_triplets[0]\n",
    "import json\n",
    "\n",
    "# Group by unique sentence (or sentence_id if preferred)\n",
    "grouped = train_df.groupby(\"sentence_text\")\n",
    "\n",
    "# Build JSON-like structure\n",
    "sentence_entities = []\n",
    "\n",
    "for sentence_text, group in grouped:\n",
    "    # Collect unique entities from both drug1 and drug2\n",
    "    entities = set(group[\"e1_text\"]).union(set(group[\"e2_text\"]))\n",
    "    \n",
    "    sentence_entities.append({\n",
    "        \"sentence\": sentence_text,\n",
    "        \"entities\": sorted(entities)  # optional: sort for consistency\n",
    "    })\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"train_df_sentence_entities.json\", \"w\") as f:\n",
    "    json.dump(sentence_entities, f, indent=2)\n",
    "\n",
    "with open(\"train_df_sentence_entities.json\",\"r\") as f:\n",
    "    train_entities = json.load(f)\n",
    "\n",
    "train_entities[0]\n",
    "train_triplets\n",
    "few_shot_examples = train_triplets\n",
    "few_shot_examples\n",
    "import json\n",
    "\n",
    "# Group by unique sentence (or sentence_id if preferred)\n",
    "grouped = test_df.groupby(\"sentence_text\")\n",
    "\n",
    "# Build JSON-like structure\n",
    "sentence_entities = []\n",
    "\n",
    "for sentence_text, group in grouped:\n",
    "    # Collect unique entities from both drug1 and drug2\n",
    "    entities = set(group[\"e1_text\"]).union(set(group[\"e2_text\"]))\n",
    "    \n",
    "    sentence_entities.append({\n",
    "        \"sentence\": sentence_text,\n",
    "        \"entities\": sorted(entities)  # optional: sort for consistency\n",
    "    })\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"test_df_sentence_entities.json\", \"w\") as f:\n",
    "    json.dump(sentence_entities, f, indent=2)\n",
    "\n",
    "with open(\"test_df_sentence_entities.json\",\"r\") as f:\n",
    "    test_entities = json.load(f)\n",
    "\n",
    "test_entities[0]\n",
    "len(train_entities)\n",
    "len(test_entities)\n",
    "examples_df\n",
    "import json\n",
    "\n",
    "# Group by sentence (assuming sentence text is unique identifier)\n",
    "grouped = examples_df.groupby(\"sentence_text\")\n",
    "\n",
    "# Build JSON-like structure\n",
    "json_data = []\n",
    "\n",
    "for sentence_text, group in grouped:\n",
    "    # Get triplets from the group\n",
    "    triplets = list(zip(group[\"e1_text\"], group[\"type\"], group[\"e2_text\"]))\n",
    "    \n",
    "    # Optional: if 'type4' column exists, get the most frequent or first type4\n",
    "    type4_value = group[\"type\"].iloc[0]\n",
    "    \n",
    "    # Create record\n",
    "    json_data.append({\n",
    "        \"sentence\": sentence_text,\n",
    "        \"interaction_type\": type4_value,\n",
    "        \"triplets\": triplets\n",
    "    })\n",
    "\n",
    "# Save to file\n",
    "with open(\"few_shot_example_triplets.json\", \"w\") as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "\n",
    "with open(\"few_shot_example_triplets.json\",\"r\") as f:\n",
    "    few_shot_entities = json.load(f)\n",
    "\n",
    "few_shot_entities[0]\n",
    "len(few_shot_entities)\n",
    "train_entities\n",
    "train_triplets # ground truth, remove interaction_type and choose random examples for few_shot\n",
    "def transform_structure(data):\n",
    "    output = {'sentence': data['sentence']}\n",
    "    for idx, entity in enumerate(data['entities'], 1):\n",
    "        output[f'Entity_{idx}'] = entity\n",
    "    return output\n",
    "\n",
    "transformed = transform_structure(train_entities[0])\n",
    "print(transformed)\n",
    "\n",
    "def transform_structure(data):\n",
    "    output = {'Sentence': data['sentence']}\n",
    "    for idx, entity in enumerate(data['entities'], 1):\n",
    "        output[f'Entity_{idx}'] = entity\n",
    "    return output\n",
    "\n",
    "updated_train_entities = []\n",
    "for i in range(len(train_entities)):\n",
    "    transformed = transform_structure(train_entities[i])\n",
    "    updated_train_entities.append(transformed)\n",
    "print(updated_train_entities[-1])\n",
    "len(updated_train_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ee609f",
   "metadata": {},
   "source": [
    "# GPT v1 improved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a3829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"random_few_shot_examples_2.json\",\"r\") as f:\n",
    "    few_shot_prompt_2_examples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300412e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"random_few_shot_examples_5.json\",\"r\") as f:\n",
    "    few_shot_prompt_5_examples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c2ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"random_few_shot_examples_10.json\",\"r\") as f:\n",
    "    few_shot_prompt_10_examples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caff8353",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"random_few_shot_examples_20.json\",\"r\") as f:\n",
    "    few_shot_prompt_20_examples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2391bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from google import genai\n",
    "\n",
    "\n",
    "GEMINI_API_KEY = ''\n",
    "assert GEMINI_API_KEY, \"\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
    "client = genai.Client()\n",
    "\n",
    "# Output holder\n",
    "all_responses = []\n",
    "# GPT model 1\n",
    "# Prompt styles\n",
    "interaction_types = \"\"\"\n",
    "- mechanism: Pharmacokinetic interactions affecting drug metabolism (e.g., \"Drug A inhibits metabolism of Drug B\")\n",
    "- effect: Pharmacodynamic effects or observed outcomes (e.g., \"Increased side effects when combining Drug A and Drug B\")\n",
    "- advice: Recommendations or contraindications (e.g., \"Avoid combining Drug A with Drug B\")\n",
    "- int: General interaction mentions without specifics (e.g., \"Interaction exists between Drug A and Drug B\")\n",
    "- no interaction: No sign of explicit of implicit interaction between given drugs\n",
    "\"\"\"\n",
    "\n",
    "few_shot_examples = f\"\"\"\n",
    "{interaction_types}\n",
    "\"\"\"\n",
    "\n",
    "def extract_entities(entity_dict):\n",
    "    return [v for k, v in entity_dict.items() if k.startswith(\"Entity_\")]\n",
    "\n",
    "# Inference loop\n",
    "for i in range(0, len(updated_train_entities)):\n",
    "    example = updated_train_entities[i]\n",
    "    sentence = example[\"Sentence\"]\n",
    "    entities = extract_entities(example)\n",
    "\n",
    "    output_entry = {\n",
    "        \"index\": i,\n",
    "        \"sentence\": sentence,\n",
    "        \"entities\": entities\n",
    "    }\n",
    "\n",
    "    # Prepare prompts\n",
    "    zero_shot_prompt = f\"\"\"You are a biomedical expert. Extract drug-drug interactions (DDIs) from the following text.\n",
    "\n",
    "Definitions of interaction types:\n",
    "- mechanism: Pharmacokinetic interaction (metabolism, absorption, clearance)\n",
    "- effect: Pharmacodynamic effect or observed clinical outcome\n",
    "- advice: Explicit recommendation, warning, or contraindication\n",
    "- int: Generic interaction mentioned without details\n",
    "- no interaction: Explicitly or implicitly no interaction\n",
    "\n",
    "Rules:\n",
    "1. Consider all possible unique pairs of the given entities.\n",
    "2. For each unique pair, output exactly one triplet with one interaction type.\n",
    "3. Do not output both (Drug1, interaction_type, Drug2) and (Drug2, interaction_type, Drug1) â€” they are the same pair. \n",
    "   Order does not matter, so include each pair only once.\n",
    "4. Output MUST be in the form:\n",
    "   (Drug1, interaction_type, Drug2)\n",
    "5. Do not include any explanation or text outside the triplets.\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\"\"\"\n",
    "\n",
    "    few_shot_prompt_2 = f\"\"\"You are a biomedical expert. Extract drug-drug interactions (DDIs) from biomedical text.\n",
    "\n",
    "Interaction types:\n",
    "- mechanism: Pharmacokinetic interaction (metabolism, absorption, clearance)\n",
    "- effect: Pharmacodynamic effect or observed clinical outcome\n",
    "- advice: Explicit recommendation, warning, or contraindication\n",
    "- int: Generic interaction mentioned without details\n",
    "- no interaction: Explicitly or implicitly no interaction\n",
    "\n",
    "Rules:\n",
    "1. Consider all possible unique pairs of the given entities.\n",
    "2. For each unique pair, output exactly one triplet with one interaction type.\n",
    "3. Do not output both (Drug1, interaction_type, Drug2) and (Drug2, interaction_type, Drug1) â€” they are the same pair. \n",
    "   Order does not matter, so include each pair only once.\n",
    "4. Output MUST be in the form:\n",
    "   (Drug1, interaction_type, Drug2)\n",
    "5. Do not include any explanation or text outside the triplets.\n",
    "\n",
    "Examples: {few_shot_prompt_2_examples}\n",
    "\n",
    "Now extract triplets for:\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\"\"\"\n",
    "    \n",
    "    few_shot_prompt_5 = f\"\"\"You are a biomedical expert. Extract drug-drug interactions (DDIs) from biomedical text.\n",
    "\n",
    "Interaction types:\n",
    "- mechanism: Pharmacokinetic interaction (metabolism, absorption, clearance)\n",
    "- effect: Pharmacodynamic effect or observed clinical outcome\n",
    "- advice: Explicit recommendation, warning, or contraindication\n",
    "- int: Generic interaction mentioned without details\n",
    "- no interaction: Explicitly or implicitly no interaction\n",
    "\n",
    "Rules:\n",
    "1. Consider all possible unique pairs of the given entities.\n",
    "2. For each unique pair, output exactly one triplet with one interaction type.\n",
    "3. Do not output both (Drug1, interaction_type, Drug2) and (Drug2, interaction_type, Drug1) â€” they are the same pair. \n",
    "   Order does not matter, so include each pair only once.\n",
    "4. Output MUST be in the form:\n",
    "   (Drug1, interaction_type, Drug2)\n",
    "5. Do not include any explanation or text outside the triplets.\n",
    "\n",
    "Examples: {few_shot_prompt_5_examples}\n",
    "\n",
    "Now extract triplets for:\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\"\"\"\n",
    "\n",
    "\n",
    "    few_shot_prompt_10 = f\"\"\"You are a biomedical expert. Extract drug-drug interactions (DDIs) from biomedical text.\n",
    "\n",
    "Interaction types:\n",
    "- mechanism: Pharmacokinetic interaction (metabolism, absorption, clearance)\n",
    "- effect: Pharmacodynamic effect or observed clinical outcome\n",
    "- advice: Explicit recommendation, warning, or contraindication\n",
    "- int: Generic interaction mentioned without details\n",
    "- no interaction: Explicitly or implicitly no interaction\n",
    "\n",
    "Rules:\n",
    "1. Consider all possible unique pairs of the given entities.\n",
    "2. For each unique pair, output exactly one triplet with one interaction type.\n",
    "3. Do not output both (Drug1, interaction_type, Drug2) and (Drug2, interaction_type, Drug1) â€” they are the same pair. \n",
    "   Order does not matter, so include each pair only once.\n",
    "4. Output MUST be in the form:\n",
    "   (Drug1, interaction_type, Drug2)\n",
    "5. Do not include any explanation or text outside the triplets.\n",
    "\n",
    "Examples: {few_shot_prompt_10_examples}\n",
    "\n",
    "Now extract triplets for:\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\"\"\"\n",
    "    \n",
    "    few_shot_prompt_20 = f\"\"\"You are a biomedical expert. Extract drug-drug interactions (DDIs) from biomedical text.\n",
    "\n",
    "Interaction types:\n",
    "- mechanism: Pharmacokinetic interaction (metabolism, absorption, clearance)\n",
    "- effect: Pharmacodynamic effect or observed clinical outcome\n",
    "- advice: Explicit recommendation, warning, or contraindication\n",
    "- int: Generic interaction mentioned without details\n",
    "- no interaction: Explicitly or implicitly no interaction\n",
    "\n",
    "Rules:\n",
    "1. Consider all possible unique pairs of the given entities.\n",
    "2. For each unique pair, output exactly one triplet with one interaction type.\n",
    "3. Do not output both (Drug1, interaction_type, Drug2) and (Drug2, interaction_type, Drug1) â€” they are the same pair. \n",
    "   Order does not matter, so include each pair only once.\n",
    "4. Output MUST be in the form:\n",
    "   (Drug1, interaction_type, Drug2)\n",
    "5. Do not include any explanation or text outside the triplets.\n",
    "\n",
    "Examples: {few_shot_prompt_20_examples}\n",
    "\n",
    "Now extract triplets for:\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\"\"\"\n",
    "\n",
    "\n",
    "    cot_prompt = f\"\"\"You are a biomedical expert. Perform the following:\n",
    "\n",
    "Step 1: List all unique entity pairs from the given entities.\n",
    "Step 2: For each unique pair, analyze the sentence context to determine if an interaction exists.\n",
    "Step 3: Classify into exactly one of:\n",
    "- mechanism\n",
    "- effect\n",
    "- advice\n",
    "- int\n",
    "- no interaction\n",
    "Step 4: Output exactly one triplet for each unique pair.\n",
    "\n",
    "Rules:\n",
    "- Do not output both (Drug1, interaction_type, Drug2) and (Drug2, interaction_type, Drug1). They are the same pair. \n",
    "  Order does not matter, so include each pair only once.\n",
    "- Output MUST be in the form:\n",
    "  (Drug1, interaction_type, Drug2)\n",
    "- Do not include any explanation or text outside the triplets.\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\n",
    "\"\"\"\n",
    "\n",
    "    prompts = {\n",
    "        \"zero_shot_output\": zero_shot_prompt,\n",
    "        \"few_shot_output_2\": few_shot_prompt_2,\n",
    "        \"few_shot_output_5\": few_shot_prompt_5,\n",
    "        \"few_shot_output_10\": few_shot_prompt_10,\n",
    "        \"few_shot_output_20\": few_shot_prompt_20,\n",
    "        \"cot_output\": cot_prompt\n",
    "    }\n",
    "\n",
    "    for method_name, prompt in prompts.items():\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemma-3-27b-it\",\n",
    "                contents=prompt,\n",
    "                config=genai.types.GenerateContentConfig(\n",
    "                    temperature=0.0\n",
    "                )\n",
    "            )\n",
    "            time.sleep(6)\n",
    "\n",
    "            output_entry[method_name] = response.text.strip()\n",
    "            print(f\"[{method_name}] Finished index {i}\")\n",
    "            print(f\"{method_name}: {response.text.strip()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {i} with method {method_name}: {e}\")\n",
    "            output_entry[method_name] = \"ERROR\"\n",
    "\n",
    "    all_responses.append(output_entry)\n",
    "\n",
    "    # Save after each example (optional safety)\n",
    "    with open(\"GPT_Prompting_method_1.json\", \"w\") as f:\n",
    "        json.dump(all_responses, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7669b22e",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56cf1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open(\"GPT_Prompting_method_1.json\", \"r\") as f:\n",
    "    all_responses = json.load(f)\n",
    "all_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d77e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplets_df = pd.DataFrame(train_triplets)\n",
    "train_triplets_df\n",
    "triplet_prediction_results = pd.DataFrame(all_responses)\n",
    "triplet_prediction_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bb2e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = train_triplets_df.merge(triplet_prediction_results)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b4952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "# Configuration\n",
    "INTERACTION_TYPES = {\"effect\", \"mechanism\", \"advice\", \"int\", \"no interaction\"}\n",
    "MODEL_MAP = {\n",
    "    'zero_shot_output': 'Zero-Shot',\n",
    "    'few_shot_output_2': 'Few-Shot-2',\n",
    "    'few_shot_output_5': 'Few-Shot-5',\n",
    "    'few_shot_output_10': 'Few-Shot-10',\n",
    "    'few_shot_output_20': 'Few-Shot-20',\n",
    "    'cot_output': 'CoT'\n",
    "}\n",
    "MODEL_ORDER = list(MODEL_MAP.keys())\n",
    "\n",
    "# --- Normalization ---\n",
    "def normalize_interaction_type(itype: str) -> str:\n",
    "    \"\"\"Map variations of interaction types to canonical form\"\"\"\n",
    "    itype = itype.strip().lower()\n",
    "    if itype in {\"advise\", \"adviced\", \"advised\", \"advice\"}:\n",
    "        return \"advice\"\n",
    "    return itype\n",
    "\n",
    "def normalize_drug_pair(drug1, drug2):\n",
    "    \"\"\"Create canonical drug pair representation\"\"\"\n",
    "    return tuple(sorted([drug1.strip().lower(), drug2.strip().lower()]))\n",
    "\n",
    "# --- Parsing ---\n",
    "def parse_prediction_output(pred_output):\n",
    "    \"\"\"Robust parser that requires exactly 2 drugs and 1 interaction type\"\"\"\n",
    "    if not isinstance(pred_output, str):\n",
    "        return []\n",
    "    \n",
    "    triplet_pattern = r'\\(([^,]+?),\\s*([^,]+?),\\s*([^)]+?)\\)'\n",
    "    raw_triplets = re.findall(triplet_pattern, pred_output)\n",
    "    \n",
    "    cleaned_triplets = []\n",
    "    for triplet in raw_triplets:\n",
    "        parts = [p.strip().lower() for p in triplet]\n",
    "        norm_parts = [normalize_interaction_type(p) for p in parts]\n",
    "        \n",
    "        # Identify interaction type candidates\n",
    "        interaction_candidates = [p for p in norm_parts if p in INTERACTION_TYPES]\n",
    "        drug_candidates = [parts[i] for i, p in enumerate(norm_parts) if p not in INTERACTION_TYPES]\n",
    "        \n",
    "        if len(interaction_candidates) == 1 and len(drug_candidates) == 2:\n",
    "            itype = interaction_candidates[0]\n",
    "            cleaned_triplets.append((drug_candidates[0], itype, drug_candidates[1]))\n",
    "            continue\n",
    "        \n",
    "        # Fallback: positional checks\n",
    "        if len(parts) == 3:\n",
    "            if norm_parts[1] in INTERACTION_TYPES:\n",
    "                cleaned_triplets.append((parts[0], norm_parts[1], parts[2]))\n",
    "            elif norm_parts[0] in INTERACTION_TYPES:\n",
    "                cleaned_triplets.append((parts[1], norm_parts[0], parts[2]))\n",
    "            elif norm_parts[2] in INTERACTION_TYPES:\n",
    "                cleaned_triplets.append((parts[0], norm_parts[2], parts[1]))\n",
    "    \n",
    "    return cleaned_triplets\n",
    "\n",
    "\n",
    "# --- Evaluation ---\n",
    "def evaluate_predictions(df):\n",
    "    \"\"\"Comprehensive evaluation with model comparison metrics\"\"\"\n",
    "    results = []\n",
    "    per_class_results = []\n",
    "    \n",
    "    for model_col in MODEL_ORDER:\n",
    "        if model_col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        model_name = MODEL_MAP[model_col]\n",
    "        \n",
    "        # Initialize metrics\n",
    "        metrics = {\n",
    "            'Model': model_name,\n",
    "            'Accuracy': 0,\n",
    "            'Micro_Precision': 0, 'Micro_Recall': 0, 'Micro_F1': 0,\n",
    "            'Macro_Precision': 0, 'Macro_Recall': 0, 'Macro_F1': 0,\n",
    "            'Weighted_Precision': 0, 'Weighted_Recall': 0, 'Weighted_F1': 0,\n",
    "        }\n",
    "        \n",
    "        class_metrics = {itype: {'TP': 0, 'FP': 0, 'FN': 0, 'TN': 0} for itype in INTERACTION_TYPES}\n",
    "        all_true = []\n",
    "        all_pred = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Process ground truth\n",
    "            gold_triplets = set()\n",
    "            for triplet in row['triplets']:\n",
    "                if len(triplet) == 3:\n",
    "                    d1, itype, d2 = [x.strip().lower() for x in triplet]\n",
    "                    itype = normalize_interaction_type(itype)\n",
    "                    gold_triplets.add((normalize_drug_pair(d1, d2), itype))\n",
    "            \n",
    "            # Process predictions\n",
    "            pred_triplets = set()\n",
    "            for d1, itype, d2 in parse_prediction_output(row[model_col]):\n",
    "                itype = normalize_interaction_type(itype)\n",
    "                pred_triplets.add((normalize_drug_pair(d1, d2), itype))\n",
    "            \n",
    "            # Create lists for sklearn metrics\n",
    "            for itype in INTERACTION_TYPES:\n",
    "                # True positive: both gold and prediction have this interaction\n",
    "                if any((dp, t) in gold_triplets and t == itype for dp, t in pred_triplets):\n",
    "                    class_metrics[itype]['TP'] += 1\n",
    "                    all_true.append(itype)\n",
    "                    all_pred.append(itype)\n",
    "                # False positive: prediction has this interaction but gold doesn't\n",
    "                elif any(t == itype for dp, t in pred_triplets):\n",
    "                    class_metrics[itype]['FP'] += 1\n",
    "                    all_pred.append(itype)\n",
    "                    # Assign the true label\n",
    "                    true_label = \"no interaction\"\n",
    "                    for dp, t in gold_triplets:\n",
    "                        if dp in [dp_p for dp_p, t_p in pred_triplets if t_p == itype]:\n",
    "                            true_label = t\n",
    "                            break\n",
    "                    all_true.append(true_label)\n",
    "                # False negative: gold has this interaction but prediction doesn't\n",
    "                elif any(t == itype for dp, t in gold_triplets):\n",
    "                    class_metrics[itype]['FN'] += 1\n",
    "                    all_true.append(itype)\n",
    "                    all_pred.append(\"no interaction\")  # Missed prediction\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct = sum(1 for t, p in zip(all_true, all_pred) if t == p)\n",
    "        total = len(all_true) if all_true else 1\n",
    "        metrics['Accuracy'] = correct / total if total > 0 else 0\n",
    "        \n",
    "        # Calculate micro, macro, and weighted metrics using sklearn\n",
    "        if all_true and all_pred:\n",
    "            micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(\n",
    "                all_true, all_pred, average='micro', zero_division=0)\n",
    "            macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
    "                all_true, all_pred, average='macro', zero_division=0)\n",
    "            weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
    "                all_true, all_pred, average='weighted', zero_division=0)\n",
    "            \n",
    "            metrics.update({\n",
    "                'Micro_Precision': micro_precision,\n",
    "                'Micro_Recall': micro_recall,\n",
    "                'Micro_F1': micro_f1,\n",
    "                'Macro_Precision': macro_precision,\n",
    "                'Macro_Recall': macro_recall,\n",
    "                'Macro_F1': macro_f1,\n",
    "                'Weighted_Precision': weighted_precision,\n",
    "                'Weighted_Recall': weighted_recall,\n",
    "                'Weighted_F1': weighted_f1\n",
    "            })\n",
    "        \n",
    "        results.append(metrics)\n",
    "        \n",
    "        # Calculate per-class metrics\n",
    "        for itype in INTERACTION_TYPES:\n",
    "            cm = class_metrics[itype]\n",
    "            p = cm['TP'] / (cm['TP'] + cm['FP']) if cm['TP'] + cm['FP'] > 0 else 0\n",
    "            r = cm['TP'] / (cm['TP'] + cm['FN']) if cm['TP'] + cm['FN'] > 0 else 0\n",
    "            f = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "            support = cm['TP'] + cm['FN']\n",
    "            \n",
    "            per_class_results.append({\n",
    "                'Model': model_name,\n",
    "                'Interaction': itype,\n",
    "                'Precision': p,\n",
    "                'Recall': r,\n",
    "                'F1': f,\n",
    "                'Support': support\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results), pd.DataFrame(per_class_results)\n",
    "\n",
    "# --- Report ---\n",
    "def generate_comparison_report(global_df, class_df):\n",
    "    \"\"\"Create model comparison report with sorting\"\"\"\n",
    "    global_df = global_df.sort_values('Micro_F1', ascending=False)\n",
    "    \n",
    "    # Format numbers\n",
    "    global_df = global_df.round(4)\n",
    "    class_df = class_df.round(4)\n",
    "    \n",
    "    report = \"MODEL COMPARISON REPORT\\n\"\n",
    "    report += \"=======================\\n\\n\"\n",
    "    report += \"Global Metrics:\\n\"\n",
    "    report += global_df.to_string(index=False) + \"\\n\\n\"\n",
    "    report += \"Per-Class Metrics:\\n\"\n",
    "    report += class_df.to_string(index=False)\n",
    "    \n",
    "    best_model = global_df.iloc[0]\n",
    "    report += f\"\\n\\nBest Model: {best_model['Model']} (Macro F1={best_model['Macro_F1']:.4f}, Micro F1={best_model['Micro_F1']:.4f})\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    global_results, class_results = evaluate_predictions(merged_df)\n",
    "    report = generate_comparison_report(global_results, class_results)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd03061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define valid interaction types\n",
    "INTERACTION_TYPES = {\"effect\", \"mechanism\", \"advice\", \"int\", \"no interaction\"}\n",
    "\n",
    "def process_triplets(df):\n",
    "    # Step 1: Explode ground truth triplets\n",
    "    exploded_rows = []\n",
    "    for _, row in merged_df.iterrows():\n",
    "        for triplet in row['triplets']:\n",
    "            exploded_rows.append({\n",
    "                'index': row['index'],\n",
    "                'sentence': row['sentence'],\n",
    "                'ground_truth_triplet': triplet,\n",
    "                'ground_truth_type': triplet[1],  # interaction_type from triplet\n",
    "                'zero_shot_output': row['zero_shot_output'],\n",
    "                'few_shot_output_2': row['few_shot_output_2'],\n",
    "                'few_shot_output_5': row['few_shot_output_5'],\n",
    "                'few_shot_output_10': row['few_shot_output_10'],\n",
    "                'few_shot_output_20': row['few_shot_output_20'],\n",
    "                'cot_output': row['cot_output'],\n",
    "            })\n",
    "    df_expanded = pd.DataFrame(exploded_rows)\n",
    "    return df_expanded\n",
    "\n",
    "def parse_prediction_output(pred_output):\n",
    "    \"\"\"Parse prediction output into cleaned triplets with consistent format\"\"\"\n",
    "    if not isinstance(pred_output, str):\n",
    "        return []\n",
    "    \n",
    "    # Find all triplets in the string\n",
    "    triplet_pattern = r'\\(([^,]+?),\\s*([^,]+?),\\s*([^)]+?)\\)'\n",
    "    raw_triplets = re.findall(triplet_pattern, pred_output)\n",
    "    \n",
    "    cleaned_triplets = []\n",
    "    for triplet in raw_triplets:\n",
    "        parts = [p.strip() for p in triplet]\n",
    "        \n",
    "        # Case 1: Middle element is interaction type\n",
    "        if parts[1].lower() in INTERACTION_TYPES:\n",
    "            cleaned_triplets.append((parts[0], parts[1].lower(), parts[2]))\n",
    "        \n",
    "        # Case 2: Last element is interaction type\n",
    "        elif parts[2].lower() in INTERACTION_TYPES:\n",
    "            cleaned_triplets.append((parts[0], parts[2].lower(), parts[1]))\n",
    "        \n",
    "        # Case 3: Try to identify interaction by position\n",
    "        else:\n",
    "            # Try first element as interaction\n",
    "            if parts[0].lower() in INTERACTION_TYPES:\n",
    "                cleaned_triplets.append((parts[1], parts[0].lower(), parts[2]))\n",
    "            # Try last element as interaction (even if not in set)\n",
    "            elif parts[2].lower() not in INTERACTION_TYPES and parts[1].lower() not in INTERACTION_TYPES:\n",
    "                # Final fallback: assume last element is interaction\n",
    "                cleaned_triplets.append((parts[0], parts[2].lower(), parts[1]))\n",
    "    \n",
    "    return cleaned_triplets\n",
    "\n",
    "def match_predictions(df_expanded, column_name):\n",
    "    \"\"\"Match predictions to ground truth with per-type tracking\"\"\"\n",
    "    matched_data = []\n",
    "    \n",
    "    for _, row in df_expanded.iterrows():\n",
    "        # Get ground truth triplet\n",
    "        gt_drug1, gt_interaction, gt_drug2 = row['ground_truth_triplet']\n",
    "        gt_interaction = gt_interaction.lower()\n",
    "        gt_drug_pair = tuple(sorted([gt_drug1.strip(), gt_drug2.strip()]))\n",
    "        \n",
    "        # Parse predictions\n",
    "        predictions = parse_prediction_output(row[column_name])\n",
    "        pred_matches = {tuple(sorted([d1.strip(), d2.strip()])): itype \n",
    "                        for d1, itype, d2 in predictions}\n",
    "        \n",
    "        # Check match for this specific triplet\n",
    "        match_found = False\n",
    "        if gt_drug_pair in pred_matches:\n",
    "            match_found = (pred_matches[gt_drug_pair] == gt_interaction)\n",
    "        \n",
    "        matched_data.append({\n",
    "            'index': row['index'],\n",
    "            'sentence': row['sentence'],\n",
    "            'ground_truth_triplet': (gt_drug1, gt_interaction, gt_drug2),\n",
    "            'ground_truth_type': gt_interaction,\n",
    "            'predicted_triplets': predictions,\n",
    "            'match': match_found,\n",
    "            'drug_pair': gt_drug_pair\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(matched_data)\n",
    "\n",
    "# Generate matched dataframes\n",
    "df_expanded = process_triplets(merged_df)  # Ensure this creates one row per triplet\n",
    "\n",
    "matched_dfs = {\n",
    "    'zero_shot': match_predictions(df_expanded, 'zero_shot_output'),\n",
    "    'few_shot_2': match_predictions(df_expanded, 'few_shot_output_2'),\n",
    "    'few_shot_5': match_predictions(df_expanded, 'few_shot_output_5'),\n",
    "    'few_shot_10': match_predictions(df_expanded, 'few_shot_output_10'),\n",
    "    'few_shot_20': match_predictions(df_expanded, 'few_shot_output_20'),\n",
    "    'cot': match_predictions(df_expanded, 'cot_output')\n",
    "}\n",
    "\n",
    "# Calculate per-type accuracy\n",
    "results = []\n",
    "\n",
    "for method_name, df in matched_dfs.items():\n",
    "    # Overall accuracy\n",
    "    overall_acc = df['match'].mean()\n",
    "    \n",
    "    # Per-type accuracy\n",
    "    type_acc = {}\n",
    "    for itype in INTERACTION_TYPES:\n",
    "        type_df = df[df['ground_truth_type'] == itype]\n",
    "        if len(type_df) > 0:\n",
    "            type_acc[itype] = type_df['match'].mean()\n",
    "        else:\n",
    "            type_acc[itype] = None  # Handle missing types\n",
    "    \n",
    "    results.append({\n",
    "        'method': method_name,\n",
    "        'overall_accuracy': overall_acc,\n",
    "        **{f'{itype}_accuracy': acc for itype, acc in type_acc.items()}\n",
    "    })\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe12faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pair_and_interaction_counts(merged_df):\n",
    "    TECHNIQUE_ORDER = [\n",
    "        'zero_shot_output',\n",
    "        'few_shot_output_2',\n",
    "        'few_shot_output_5',\n",
    "        'few_shot_output_10',\n",
    "        'few_shot_output_20',\n",
    "        'cot_output'\n",
    "    ]\n",
    "\n",
    "    TECHNIQUE_MAP = {\n",
    "        'zero_shot_output': 'Zero-Shot',\n",
    "        'few_shot_output_2': 'Few-Shot (2)',\n",
    "        'few_shot_output_5': 'Few-Shot (5)',\n",
    "        'few_shot_output_10': 'Few-Shot (10)',\n",
    "        'few_shot_output_20': 'Few-Shot (20)',\n",
    "        'cot_output': 'Chain-of-Thought'\n",
    "    }\n",
    "    \n",
    "    per_method_details = []\n",
    "    per_interaction_details = []\n",
    "    \n",
    "    for tech_col in TECHNIQUE_ORDER:\n",
    "        if tech_col not in merged_df.columns:\n",
    "            continue\n",
    "        \n",
    "        tech_name = TECHNIQUE_MAP.get(tech_col, tech_col)\n",
    "        \n",
    "        interaction_correct, interaction_total = 0, 0\n",
    "        per_class = {itype: {'tp': 0, 'fn': 0} for itype in INTERACTION_TYPES}\n",
    "        \n",
    "        for _, row in merged_df.iterrows():\n",
    "            # Ground truth triplets\n",
    "            gold_triplets = set()\n",
    "            if isinstance(row['triplets'], list):\n",
    "                for triplet in row['triplets']:\n",
    "                    if len(triplet) == 3:\n",
    "                        d1, itype, d2 = [x.strip().lower() for x in triplet]\n",
    "                        itype = normalize_interaction_type(itype)  # <-- FIX\n",
    "                        gold_triplets.add((normalize_drug_pair(d1, d2), itype))\n",
    "            \n",
    "            # Predictions\n",
    "            pred_triplets = set()\n",
    "            for d1, itype, d2 in parse_prediction_output(row[tech_col]):\n",
    "                itype = normalize_interaction_type(itype)  # <-- FIX\n",
    "                pred_triplets.add((normalize_drug_pair(d1, d2), itype))\n",
    "            \n",
    "            # Pair-level counts (correct if both pair+interaction match)\n",
    "            gold_pairs = {dp for dp, _ in gold_triplets}\n",
    "            for dp, it in pred_triplets:\n",
    "                if dp in gold_pairs:  # model predicted some interaction for a true pair\n",
    "                    interaction_total += 1\n",
    "                    if (dp, it) in gold_triplets:\n",
    "                        interaction_correct += 1\n",
    "            \n",
    "            # Per-interaction type counts\n",
    "            for itype in INTERACTION_TYPES:\n",
    "                gold_sub = {(dp, it) for dp, it in gold_triplets if it == itype}\n",
    "                pred_sub = {(dp, it) for dp, it in pred_triplets if it == itype}\n",
    "                \n",
    "                tp = len(gold_sub & pred_sub)\n",
    "                fn = len(gold_sub - pred_sub)\n",
    "                \n",
    "                per_class[itype]['tp'] += tp\n",
    "                per_class[itype]['fn'] += fn\n",
    "        \n",
    "        # Store per-method totals\n",
    "        per_method_details.append({\n",
    "            \"Technique\": tech_name,\n",
    "            \"Correct Pairs\": interaction_correct,\n",
    "            \"Total Pairs\": interaction_total\n",
    "        })\n",
    "        \n",
    "        # Store per-interaction type totals\n",
    "        for itype in INTERACTION_TYPES:\n",
    "            tp = per_class[itype]['tp']\n",
    "            fn = per_class[itype]['fn']\n",
    "            per_interaction_details.append({\n",
    "                \"Technique\": tech_name,\n",
    "                \"Interaction Type\": itype,\n",
    "                \"Correct\": tp,\n",
    "                \"Total\": tp + fn\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(per_method_details), pd.DataFrame(per_interaction_details)\n",
    "method_df, interaction_df = calculate_pair_and_interaction_counts(merged_df)\n",
    "print(\"PER-METHOD PAIR COUNTS:\")\n",
    "print(method_df.to_string(index=False))\n",
    "print(\"\\nPER-INTERACTION TYPE COUNTS:\")\n",
    "print(interaction_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89300bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_class_distributions(class_df):\n",
    "    \"\"\"\n",
    "    Visualize correct prediction distribution per class across models.\n",
    "    Each bar shows recall (correct / total).\n",
    "    \"\"\"\n",
    "    classes = sorted(class_df['Interaction'].unique())\n",
    "    models = class_df['Model'].unique()\n",
    "\n",
    "    # Make one subplot per interaction type\n",
    "    fig, axes = plt.subplots(len(classes), 1, figsize=(10, 4 * len(classes)), sharex=True)\n",
    "\n",
    "    if len(classes) == 1:\n",
    "        axes = [axes]  # Ensure iterable\n",
    "\n",
    "    for ax, itype in zip(axes, classes):\n",
    "        subset = class_df[class_df['Interaction'] == itype]\n",
    "\n",
    "        recalls = subset.set_index('Model')['Recall']\n",
    "        supports = subset.set_index('Model')['Support']\n",
    "\n",
    "        # Plot recall bars\n",
    "        recalls.plot(kind='bar', ax=ax, color='purple', edgecolor='black')\n",
    "\n",
    "        # Annotate bars with \"TP / Support\"\n",
    "        for i, model in enumerate(recalls.index):\n",
    "            tp = int(round(recalls[model] * supports[model]))\n",
    "            ax.text(i, recalls[model] + 0.02, f\"{tp}/{supports[model]}\", \n",
    "                    ha='center', fontsize=9, rotation=0)\n",
    "\n",
    "        ax.set_ylim(0, 1.05)\n",
    "        ax.set_title(f\"Class: {itype}\", fontsize=14)\n",
    "        ax.set_ylabel(\"Recall (Correct / Total)\")\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    plt.suptitle(\"Per-Class Prediction Distribution per Model\", fontsize=16, y=1.02)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98192592",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    global_results, class_results = evaluate_predictions(merged_df)\n",
    "    report = generate_comparison_report(global_results, class_results)\n",
    "    print(report)\n",
    "\n",
    "    # ðŸ‘‡ Add this line to display the plots\n",
    "    plot_class_distributions(class_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58247691",
   "metadata": {},
   "source": [
    "# GPT v1 predictions on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c788d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_structure(data):\n",
    "    output = {'sentence': data['sentence']}\n",
    "    for idx, entity in enumerate(data['entities'], 1):\n",
    "        output[f'Entity_{idx}'] = entity\n",
    "    return output\n",
    "\n",
    "transformed = transform_structure(test_entities[0])\n",
    "print(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4c9eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_structure(data):\n",
    "    output = {'Sentence': data['sentence']}\n",
    "    for idx, entity in enumerate(data['entities'], 1):\n",
    "        output[f'Entity_{idx}'] = entity\n",
    "    return output\n",
    "\n",
    "updated_test_entities = []\n",
    "for i in range(len(test_entities)):\n",
    "    transformed = transform_structure(test_entities[i])\n",
    "    updated_test_entities.append(transformed)\n",
    "print(updated_test_entities[-1])\n",
    "len(updated_test_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"random_few_shot_examples_2.json\",\"r\") as f:\n",
    "    few_shot_prompt_2_examples = json.load(f)\n",
    "print(few_shot_prompt_2_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4f336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"random_few_shot_examples_5.json\",\"r\") as f:\n",
    "    few_shot_prompt_5_examples = json.load(f)\n",
    "print(few_shot_prompt_5_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52a263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"random_few_shot_examples_10.json\",\"r\") as f:\n",
    "    few_shot_prompt_10_examples = json.load(f)\n",
    "print(few_shot_prompt_10_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95058112",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"random_few_shot_examples_20.json\",\"r\") as f:\n",
    "    few_shot_prompt_20_examples = json.load(f)\n",
    "print(few_shot_prompt_20_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2686e378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from google import genai\n",
    "\n",
    "\n",
    "GEMINI_API_KEY = ''\n",
    "assert GEMINI_API_KEY, \"\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
    "client = genai.Client()\n",
    "\n",
    "# Output holder\n",
    "#all_responses = []\n",
    "\n",
    "# Prompt styles\n",
    "interaction_types = \"\"\"\n",
    "- mechanism: Pharmacokinetic interactions affecting drug metabolism (e.g., \"Drug A inhibits metabolism of Drug B\")\n",
    "- effect: Pharmacodynamic effects or observed outcomes (e.g., \"Increased side effects when combining Drug A and Drug B\")\n",
    "- advice: Recommendations or contraindications (e.g., \"Avoid combining Drug A with Drug B\")\n",
    "- int: General interaction mentions without specifics (e.g., \"Interaction exists between Drug A and Drug B\")\n",
    "- no interaction: No sign of explicit of implicit interaction between given drugs\n",
    "\"\"\"\n",
    "\n",
    "few_shot_examples = f\"\"\"\n",
    "{interaction_types}\n",
    "\"\"\"\n",
    "\n",
    "def extract_entities(entity_dict):\n",
    "    return [v for k, v in entity_dict.items() if k.startswith(\"Entity_\")]\n",
    "\n",
    "# Inference loop\n",
    "for i in range(0, len(updated_test_entities)):\n",
    "    example = updated_test_entities[i]\n",
    "    sentence = example[\"Sentence\"]\n",
    "    entities = extract_entities(example)\n",
    "\n",
    "    output_entry = {\n",
    "        \"index\": i,\n",
    "        \"sentence\": sentence,\n",
    "        \"entities\": entities\n",
    "    }\n",
    "\n",
    "    # Prepare prompts\n",
    "    zero_shot_prompt = f\"\"\"You are a biomedical expert. Extract drug-drug interactions (DDIs) from the following text.\n",
    "\n",
    "Definitions of interaction types:\n",
    "- mechanism: Pharmacokinetic interaction (metabolism, absorption, clearance)\n",
    "- effect: Pharmacodynamic effect or observed clinical outcome\n",
    "- advice: Explicit recommendation, warning, or contraindication,advise\n",
    "- int: Generic interaction mentioned without details\n",
    "- no interaction: Explicitly or implicitly no interaction\n",
    "\n",
    "Rules:\n",
    "1. Consider all possible unique pairs of the given entities.\n",
    "2. For each unique pair, output exactly one triplet with one interaction type.\n",
    "3. Do not output both (Drug1, interaction_type, Drug2) and (Drug2, interaction_type, Drug1) â€” they are the same pair. \n",
    "   Order does not matter, so include each pair only once.\n",
    "4. Output MUST be in the form:\n",
    "   (Drug1, interaction_type, Drug2)\n",
    "5. Do not include any explanation or text outside the triplets.\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\"\"\"\n",
    "\n",
    "    few_shot_prompt_2 = f\"\"\"You are a biomedical expert. Extract drug-drug interactions (DDIs) from biomedical text.\n",
    "\n",
    "Interaction types:\n",
    "- mechanism: Pharmacokinetic interaction (metabolism, absorption, clearance)\n",
    "- effect: Pharmacodynamic effect or observed clinical outcome\n",
    "- advice: Explicit recommendation, warning, or contraindication, advise\n",
    "- int: Generic interaction mentioned without details\n",
    "- no interaction: Explicitly or implicitly no interaction\n",
    "\n",
    "Rules:\n",
    "1. Consider all possible unique pairs of the given entities.\n",
    "2. For each unique pair, output exactly one triplet with one interaction type.\n",
    "3. Do not output both (Drug1, interaction_type, Drug2) and (Drug2, interaction_type, Drug1) â€” they are the same pair. \n",
    "   Order does not matter, so include each pair only once.\n",
    "4. Output MUST be in the form:\n",
    "   (Drug1, interaction_type, Drug2)\n",
    "5. Do not include any explanation or text outside the triplets.\n",
    "\n",
    "Examples: {few_shot_prompt_2_examples}\n",
    "\n",
    "Now extract triplets for:\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\"\"\"\n",
    "    \n",
    "    few_shot_prompt_5 = f\"\"\"You are a biomedical expert. Extract drug-drug interactions (DDIs) from biomedical text.\n",
    "\n",
    "Interaction types:\n",
    "- mechanism: Pharmacokinetic interaction (metabolism, absorption, clearance)\n",
    "- effect: Pharmacodynamic effect or observed clinical outcome\n",
    "- advice: Explicit recommendation, warning, or contraindication, advise\n",
    "- int: Generic interaction mentioned without details\n",
    "- no interaction: Explicitly or implicitly no interaction\n",
    "\n",
    "Rules:\n",
    "1. Consider all possible unique pairs of the given entities.\n",
    "2. For each unique pair, output exactly one triplet with one interaction type.\n",
    "3. Do not output both (Drug1, interaction_type, Drug2) and (Drug2, interaction_type, Drug1) â€” they are the same pair. \n",
    "   Order does not matter, so include each pair only once.\n",
    "4. Output MUST be in the form:\n",
    "   (Drug1, interaction_type, Drug2)\n",
    "5. Do not include any explanation or text outside the triplets.\n",
    "\n",
    "Examples: {few_shot_prompt_5_examples}\n",
    "\n",
    "Now extract triplets for:\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\"\"\"\n",
    "\n",
    "\n",
    "    few_shot_prompt_10 = f\"\"\"You are a biomedical expert. Extract drug-drug interactions (DDIs) from biomedical text.\n",
    "\n",
    "Interaction types:\n",
    "- mechanism: Pharmacokinetic interaction (metabolism, absorption, clearance)\n",
    "- effect: Pharmacodynamic effect or observed clinical outcome\n",
    "- advice: Explicit recommendation, warning, or contraindication, advise\n",
    "- int: Generic interaction mentioned without details\n",
    "- no interaction: Explicitly or implicitly no interaction\n",
    "\n",
    "Rules:\n",
    "1. Consider all possible unique pairs of the given entities.\n",
    "2. For each unique pair, output exactly one triplet with one interaction type.\n",
    "3. Do not output both (Drug1, interaction_type, Drug2) and (Drug2, interaction_type, Drug1) â€” they are the same pair. \n",
    "   Order does not matter, so include each pair only once.\n",
    "4. Output MUST be in the form:\n",
    "   (Drug1, interaction_type, Drug2)\n",
    "5. Do not include any explanation or text outside the triplets.\n",
    "\n",
    "Examples: {few_shot_prompt_10_examples}\n",
    "\n",
    "Now extract triplets for:\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\"\"\"\n",
    "    \n",
    "    few_shot_prompt_20 = f\"\"\"You are a biomedical expert. Extract drug-drug interactions (DDIs) from biomedical text.\n",
    "\n",
    "Interaction types:\n",
    "- mechanism: Pharmacokinetic interaction (metabolism, absorption, clearance)\n",
    "- effect: Pharmacodynamic effect or observed clinical outcome\n",
    "- advice: Explicit recommendation, warning, or contraindication, advise\n",
    "- int: Generic interaction mentioned without details\n",
    "- no interaction: Explicitly or implicitly no interaction\n",
    "\n",
    "Rules:\n",
    "1. Consider all possible unique pairs of the given entities.\n",
    "2. For each unique pair, output exactly one triplet with one interaction type.\n",
    "3. Do not output both (Drug1, interaction_type, Drug2) and (Drug2, interaction_type, Drug1) â€” they are the same pair. \n",
    "   Order does not matter, so include each pair only once.\n",
    "4. Output MUST be in the form:\n",
    "   (Drug1, interaction_type, Drug2)\n",
    "5. Do not include any explanation or text outside the triplets.\n",
    "\n",
    "Examples: {few_shot_prompt_20_examples}\n",
    "\n",
    "Now extract triplets for:\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\"\"\"\n",
    "\n",
    "\n",
    "    cot_prompt = f\"\"\"You are a biomedical expert. Perform the following:\n",
    "\n",
    "Step 1: List all unique entity pairs from the given entities.\n",
    "Step 2: For each unique pair, analyze the sentence context to determine if an interaction exists.\n",
    "Step 3: Classify into exactly one of:\n",
    "- mechanism\n",
    "- effect\n",
    "- advice\n",
    "- int\n",
    "- no interaction\n",
    "Step 4: Output exactly one triplet for each unique pair.\n",
    "\n",
    "Rules:\n",
    "- Do not output both (Drug1, interaction_type, Drug2) and (Drug2, interaction_type, Drug1). They are the same pair. \n",
    "  Order does not matter, so include each pair only once.\n",
    "- Output MUST be in the form:\n",
    "  (Drug1, interaction_type, Drug2)\n",
    "- Do not include any explanation or text outside the triplets.\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\n",
    "\"\"\"\n",
    "\n",
    "    prompts = {\n",
    "        \"zero_shot_output\": zero_shot_prompt,\n",
    "        \"few_shot_output_2\": few_shot_prompt_2,\n",
    "        \"few_shot_output_5\": few_shot_prompt_5,\n",
    "        \"few_shot_output_10\": few_shot_prompt_10,\n",
    "        \"few_shot_output_20\": few_shot_prompt_20,\n",
    "        \"cot_output\": cot_prompt\n",
    "    }\n",
    "\n",
    "    for method_name, prompt in prompts.items():\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemma-3-27b-it\",\n",
    "                contents=prompt,\n",
    "                config=genai.types.GenerateContentConfig(\n",
    "                    temperature=0.0\n",
    "                )\n",
    "            )\n",
    "            time.sleep(6)\n",
    "\n",
    "            output_entry[method_name] = response.text.strip()\n",
    "            print(f\"[{method_name}] Finished index {i}\")\n",
    "            print(f\"{method_name}: {response.text.strip()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {i} with method {method_name}: {e}\")\n",
    "            output_entry[method_name] = \"ERROR\"\n",
    "\n",
    "    all_responses.append(output_entry)\n",
    "\n",
    "    # Save after each example (optional safety)\n",
    "    with open(\"predictions_with_gpt_test.json\", \"w\") as f:\n",
    "        json.dump(all_responses, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05eb44b",
   "metadata": {},
   "source": [
    "# Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd42ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"predictions_with_gpt_test.json\", \"r\") as f:\n",
    "    all_responses = json.load(f)\n",
    "all_responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a77aa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_triplets_df = pd.DataFrame(test_triplets)\n",
    "test_triplets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36470f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_prediction_results = pd.DataFrame(all_responses)\n",
    "triplet_prediction_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb250be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = test_triplets_df.merge(triplet_prediction_results)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fd475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "# Configuration\n",
    "INTERACTION_TYPES = {\"effect\", \"mechanism\", \"advice\", \"int\", \"no interaction\"}\n",
    "MODEL_MAP = {\n",
    "    'zero_shot_output': 'Zero-Shot',\n",
    "    'few_shot_output_2': 'Few-Shot-2',\n",
    "    'few_shot_output_5': 'Few-Shot-5',\n",
    "    'few_shot_output_10': 'Few-Shot-10',\n",
    "    'few_shot_output_20': 'Few-Shot-20',\n",
    "    'cot_output': 'CoT'\n",
    "}\n",
    "MODEL_ORDER = list(MODEL_MAP.keys())\n",
    "\n",
    "# --- Normalization ---\n",
    "def normalize_interaction_type(itype: str) -> str:\n",
    "    \"\"\"Map variations of interaction types to canonical form\"\"\"\n",
    "    itype = itype.strip().lower()\n",
    "    if itype in {\"advise\", \"adviced\", \"advised\", \"advice\"}:\n",
    "        return \"advice\"\n",
    "    return itype\n",
    "\n",
    "def normalize_drug_pair(drug1, drug2):\n",
    "    \"\"\"Create canonical drug pair representation\"\"\"\n",
    "    return tuple(sorted([drug1.strip().lower(), drug2.strip().lower()]))\n",
    "\n",
    "# --- Parsing ---\n",
    "def parse_prediction_output(pred_output):\n",
    "    \"\"\"Robust parser that requires exactly 2 drugs and 1 interaction type\"\"\"\n",
    "    if not isinstance(pred_output, str):\n",
    "        return []\n",
    "    \n",
    "    triplet_pattern = r'\\(([^,]+?),\\s*([^,]+?),\\s*([^)]+?)\\)'\n",
    "    raw_triplets = re.findall(triplet_pattern, pred_output)\n",
    "    \n",
    "    cleaned_triplets = []\n",
    "    for triplet in raw_triplets:\n",
    "        parts = [p.strip().lower() for p in triplet]\n",
    "        norm_parts = [normalize_interaction_type(p) for p in parts]\n",
    "        \n",
    "        # Identify interaction type candidates\n",
    "        interaction_candidates = [p for p in norm_parts if p in INTERACTION_TYPES]\n",
    "        drug_candidates = [parts[i] for i, p in enumerate(norm_parts) if p not in INTERACTION_TYPES]\n",
    "        \n",
    "        if len(interaction_candidates) == 1 and len(drug_candidates) == 2:\n",
    "            itype = interaction_candidates[0]\n",
    "            cleaned_triplets.append((drug_candidates[0], itype, drug_candidates[1]))\n",
    "            continue\n",
    "        \n",
    "        # Fallback: positional checks\n",
    "        if len(parts) == 3:\n",
    "            if norm_parts[1] in INTERACTION_TYPES:\n",
    "                cleaned_triplets.append((parts[0], norm_parts[1], parts[2]))\n",
    "            elif norm_parts[0] in INTERACTION_TYPES:\n",
    "                cleaned_triplets.append((parts[1], norm_parts[0], parts[2]))\n",
    "            elif norm_parts[2] in INTERACTION_TYPES:\n",
    "                cleaned_triplets.append((parts[0], norm_parts[2], parts[1]))\n",
    "    \n",
    "    return cleaned_triplets\n",
    "\n",
    "\n",
    "# --- Evaluation ---\n",
    "def evaluate_predictions(df):\n",
    "    \"\"\"Comprehensive evaluation with model comparison metrics\"\"\"\n",
    "    results = []\n",
    "    per_class_results = []\n",
    "    \n",
    "    for model_col in MODEL_ORDER:\n",
    "        if model_col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        model_name = MODEL_MAP[model_col]\n",
    "        \n",
    "        # Initialize metrics\n",
    "        metrics = {\n",
    "            'Model': model_name,\n",
    "            'Accuracy': 0,\n",
    "            'Micro_Precision': 0, 'Micro_Recall': 0, 'Micro_F1': 0,\n",
    "            'Macro_Precision': 0, 'Macro_Recall': 0, 'Macro_F1': 0,\n",
    "            'Weighted_Precision': 0, 'Weighted_Recall': 0, 'Weighted_F1': 0,\n",
    "        }\n",
    "        \n",
    "        class_metrics = {itype: {'TP': 0, 'FP': 0, 'FN': 0, 'TN': 0} for itype in INTERACTION_TYPES}\n",
    "        all_true = []\n",
    "        all_pred = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Process ground truth\n",
    "            gold_triplets = set()\n",
    "            for triplet in row['triplets']:\n",
    "                if len(triplet) == 3:\n",
    "                    d1, itype, d2 = [x.strip().lower() for x in triplet]\n",
    "                    itype = normalize_interaction_type(itype)\n",
    "                    gold_triplets.add((normalize_drug_pair(d1, d2), itype))\n",
    "            \n",
    "            # Process predictions\n",
    "            pred_triplets = set()\n",
    "            for d1, itype, d2 in parse_prediction_output(row[model_col]):\n",
    "                itype = normalize_interaction_type(itype)\n",
    "                pred_triplets.add((normalize_drug_pair(d1, d2), itype))\n",
    "            \n",
    "            # Create lists for sklearn metrics\n",
    "            for itype in INTERACTION_TYPES:\n",
    "                # True positive: both gold and prediction have this interaction\n",
    "                if any((dp, t) in gold_triplets and t == itype for dp, t in pred_triplets):\n",
    "                    class_metrics[itype]['TP'] += 1\n",
    "                    all_true.append(itype)\n",
    "                    all_pred.append(itype)\n",
    "                # False positive: prediction has this interaction but gold doesn't\n",
    "                elif any(t == itype for dp, t in pred_triplets):\n",
    "                    class_metrics[itype]['FP'] += 1\n",
    "                    all_pred.append(itype)\n",
    "                    # Assign the true label\n",
    "                    true_label = \"no interaction\"\n",
    "                    for dp, t in gold_triplets:\n",
    "                        if dp in [dp_p for dp_p, t_p in pred_triplets if t_p == itype]:\n",
    "                            true_label = t\n",
    "                            break\n",
    "                    all_true.append(true_label)\n",
    "                # False negative: gold has this interaction but prediction doesn't\n",
    "                elif any(t == itype for dp, t in gold_triplets):\n",
    "                    class_metrics[itype]['FN'] += 1\n",
    "                    all_true.append(itype)\n",
    "                    all_pred.append(\"no interaction\")  # Missed prediction\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct = sum(1 for t, p in zip(all_true, all_pred) if t == p)\n",
    "        total = len(all_true) if all_true else 1\n",
    "        metrics['Accuracy'] = correct / total if total > 0 else 0\n",
    "        \n",
    "        # Calculate micro, macro, and weighted metrics using sklearn\n",
    "        if all_true and all_pred:\n",
    "            micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(\n",
    "                all_true, all_pred, average='micro', zero_division=0)\n",
    "            macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
    "                all_true, all_pred, average='macro', zero_division=0)\n",
    "            weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
    "                all_true, all_pred, average='weighted', zero_division=0)\n",
    "            \n",
    "            metrics.update({\n",
    "                'Micro_Precision': micro_precision,\n",
    "                'Micro_Recall': micro_recall,\n",
    "                'Micro_F1': micro_f1,\n",
    "                'Macro_Precision': macro_precision,\n",
    "                'Macro_Recall': macro_recall,\n",
    "                'Macro_F1': macro_f1,\n",
    "                'Weighted_Precision': weighted_precision,\n",
    "                'Weighted_Recall': weighted_recall,\n",
    "                'Weighted_F1': weighted_f1\n",
    "            })\n",
    "        \n",
    "        results.append(metrics)\n",
    "        \n",
    "        # Calculate per-class metrics\n",
    "        for itype in INTERACTION_TYPES:\n",
    "            cm = class_metrics[itype]\n",
    "            p = cm['TP'] / (cm['TP'] + cm['FP']) if cm['TP'] + cm['FP'] > 0 else 0\n",
    "            r = cm['TP'] / (cm['TP'] + cm['FN']) if cm['TP'] + cm['FN'] > 0 else 0\n",
    "            f = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "            support = cm['TP'] + cm['FN']\n",
    "            \n",
    "            per_class_results.append({\n",
    "                'Model': model_name,\n",
    "                'Interaction': itype,\n",
    "                'Precision': p,\n",
    "                'Recall': r,\n",
    "                'F1': f,\n",
    "                'Support': support\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results), pd.DataFrame(per_class_results)\n",
    "\n",
    "# --- Report ---\n",
    "def generate_comparison_report(global_df, class_df):\n",
    "    \"\"\"Create model comparison report with sorting\"\"\"\n",
    "    global_df = global_df.sort_values('Micro_F1', ascending=False)\n",
    "    \n",
    "    # Format numbers\n",
    "    global_df = global_df.round(4)\n",
    "    class_df = class_df.round(4)\n",
    "    \n",
    "    report = \"MODEL COMPARISON REPORT\\n\"\n",
    "    report += \"=======================\\n\\n\"\n",
    "    report += \"Global Metrics:\\n\"\n",
    "    report += global_df.to_string(index=False) + \"\\n\\n\"\n",
    "    report += \"Per-Class Metrics:\\n\"\n",
    "    report += class_df.to_string(index=False)\n",
    "    \n",
    "    best_model = global_df.iloc[0]\n",
    "    report += f\"\\n\\nBest Model: {best_model['Model']} (Macro F1={best_model['Macro_F1']:.4f}, Micro F1={best_model['Micro_F1']:.4f})\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    global_results, class_results = evaluate_predictions(merged_df)\n",
    "    report = generate_comparison_report(global_results, class_results)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bd8150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_class_distributions(class_df):\n",
    "    \"\"\"\n",
    "    Visualize correct prediction distribution per class across models.\n",
    "    Each bar shows recall (correct / total).\n",
    "    \"\"\"\n",
    "    classes = sorted(class_df['Interaction'].unique())\n",
    "    models = class_df['Model'].unique()\n",
    "\n",
    "    # Make one subplot per interaction type\n",
    "    fig, axes = plt.subplots(len(classes), 1, figsize=(10, 4 * len(classes)), sharex=True)\n",
    "\n",
    "    if len(classes) == 1:\n",
    "        axes = [axes]  # Ensure iterable\n",
    "\n",
    "    for ax, itype in zip(axes, classes):\n",
    "        subset = class_df[class_df['Interaction'] == itype]\n",
    "\n",
    "        recalls = subset.set_index('Model')['Recall']\n",
    "        supports = subset.set_index('Model')['Support']\n",
    "\n",
    "        # Plot recall bars\n",
    "        recalls.plot(kind='bar', ax=ax, color='skyblue', edgecolor='black')\n",
    "\n",
    "        # Annotate bars with \"TP / Support\"\n",
    "        for i, model in enumerate(recalls.index):\n",
    "            tp = int(round(recalls[model] * supports[model]))\n",
    "            ax.text(i, recalls[model] + 0.02, f\"{tp}/{supports[model]}\", \n",
    "                    ha='center', fontsize=9, rotation=0)\n",
    "\n",
    "        ax.set_ylim(0, 1.05)\n",
    "        ax.set_title(f\"Class: {itype}\", fontsize=14)\n",
    "        ax.set_ylabel(\"Recall (Correct / Total)\")\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    plt.suptitle(\"Per-Class Prediction Distribution per Model\", fontsize=16, y=1.02)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a25fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    global_results, class_results = evaluate_predictions(merged_df)\n",
    "    report = generate_comparison_report(global_results, class_results)\n",
    "    print(report)\n",
    "\n",
    "    # ðŸ‘‡ Add this line to display the plots\n",
    "    plot_class_distributions(class_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20467fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define valid interaction types\n",
    "INTERACTION_TYPES = {\"effect\", \"mechanism\", \"advice\", \"int\", \"no interaction\"}\n",
    "\n",
    "def process_triplets(df):\n",
    "    # Step 1: Explode ground truth triplets\n",
    "    exploded_rows = []\n",
    "    for _, row in merged_df.iterrows():\n",
    "        for triplet in row['triplets']:\n",
    "            exploded_rows.append({\n",
    "                'index': row['index'],\n",
    "                'sentence': row['sentence'],\n",
    "                'ground_truth_triplet': triplet,\n",
    "                'ground_truth_type': triplet[1],  # interaction_type from triplet\n",
    "                'zero_shot_output': row['zero_shot_output'],\n",
    "                'few_shot_output_2': row['few_shot_output_2'],\n",
    "                'few_shot_output_5': row['few_shot_output_5'],\n",
    "                'few_shot_output_10': row['few_shot_output_10'],\n",
    "                'few_shot_output_20': row['few_shot_output_20'],\n",
    "                'cot_output': row['cot_output'],\n",
    "            })\n",
    "    df_expanded = pd.DataFrame(exploded_rows)\n",
    "    return df_expanded\n",
    "\n",
    "def parse_prediction_output(pred_output):\n",
    "    \"\"\"Parse prediction output into cleaned triplets with consistent format\"\"\"\n",
    "    if not isinstance(pred_output, str):\n",
    "        return []\n",
    "    \n",
    "    # Find all triplets in the string\n",
    "    triplet_pattern = r'\\(([^,]+?),\\s*([^,]+?),\\s*([^)]+?)\\)'\n",
    "    raw_triplets = re.findall(triplet_pattern, pred_output)\n",
    "    \n",
    "    cleaned_triplets = []\n",
    "    for triplet in raw_triplets:\n",
    "        parts = [p.strip() for p in triplet]\n",
    "        \n",
    "        # Case 1: Middle element is interaction type\n",
    "        if parts[1].lower() in INTERACTION_TYPES:\n",
    "            cleaned_triplets.append((parts[0], parts[1].lower(), parts[2]))\n",
    "        \n",
    "        # Case 2: Last element is interaction type\n",
    "        elif parts[2].lower() in INTERACTION_TYPES:\n",
    "            cleaned_triplets.append((parts[0], parts[2].lower(), parts[1]))\n",
    "        \n",
    "        # Case 3: Try to identify interaction by position\n",
    "        else:\n",
    "            # Try first element as interaction\n",
    "            if parts[0].lower() in INTERACTION_TYPES:\n",
    "                cleaned_triplets.append((parts[1], parts[0].lower(), parts[2]))\n",
    "            # Try last element as interaction (even if not in set)\n",
    "            elif parts[2].lower() not in INTERACTION_TYPES and parts[1].lower() not in INTERACTION_TYPES:\n",
    "                # Final fallback: assume last element is interaction\n",
    "                cleaned_triplets.append((parts[0], parts[2].lower(), parts[1]))\n",
    "    \n",
    "    return cleaned_triplets\n",
    "\n",
    "def match_predictions(df_expanded, column_name):\n",
    "    \"\"\"Match predictions to ground truth with per-type tracking\"\"\"\n",
    "    matched_data = []\n",
    "    \n",
    "    for _, row in df_expanded.iterrows():\n",
    "        # Get ground truth triplet\n",
    "        gt_drug1, gt_interaction, gt_drug2 = row['ground_truth_triplet']\n",
    "        gt_interaction = gt_interaction.lower()\n",
    "        gt_drug_pair = tuple(sorted([gt_drug1.strip(), gt_drug2.strip()]))\n",
    "        \n",
    "        # Parse predictions\n",
    "        predictions = parse_prediction_output(row[column_name])\n",
    "        pred_matches = {tuple(sorted([d1.strip(), d2.strip()])): itype \n",
    "                        for d1, itype, d2 in predictions}\n",
    "        \n",
    "        # Check match for this specific triplet\n",
    "        match_found = False\n",
    "        if gt_drug_pair in pred_matches:\n",
    "            match_found = (pred_matches[gt_drug_pair] == gt_interaction)\n",
    "        \n",
    "        matched_data.append({\n",
    "            'index': row['index'],\n",
    "            'sentence': row['sentence'],\n",
    "            'ground_truth_triplet': (gt_drug1, gt_interaction, gt_drug2),\n",
    "            'ground_truth_type': gt_interaction,\n",
    "            'predicted_triplets': predictions,\n",
    "            'match': match_found,\n",
    "            'drug_pair': gt_drug_pair\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(matched_data)\n",
    "\n",
    "# Generate matched dataframes\n",
    "df_expanded = process_triplets(merged_df)  # Ensure this creates one row per triplet\n",
    "\n",
    "matched_dfs = {\n",
    "    'zero_shot': match_predictions(df_expanded, 'zero_shot_output'),\n",
    "    'few_shot_2': match_predictions(df_expanded, 'few_shot_output_2'),\n",
    "    'few_shot_5': match_predictions(df_expanded, 'few_shot_output_5'),\n",
    "    'few_shot_10': match_predictions(df_expanded, 'few_shot_output_10'),\n",
    "    'few_shot_20': match_predictions(df_expanded, 'few_shot_output_20'),\n",
    "    'cot': match_predictions(df_expanded, 'cot_output')\n",
    "}\n",
    "\n",
    "# Calculate per-type accuracy\n",
    "results = []\n",
    "\n",
    "for method_name, df in matched_dfs.items():\n",
    "    # Overall accuracy\n",
    "    overall_acc = df['match'].mean()\n",
    "    \n",
    "    # Per-type accuracy\n",
    "    type_acc = {}\n",
    "    for itype in INTERACTION_TYPES:\n",
    "        type_df = df[df['ground_truth_type'] == itype]\n",
    "        if len(type_df) > 0:\n",
    "            type_acc[itype] = type_df['match'].mean()\n",
    "        else:\n",
    "            type_acc[itype] = None  # Handle missing types\n",
    "    \n",
    "    results.append({\n",
    "        'method': method_name,\n",
    "        'overall_accuracy': overall_acc,\n",
    "        **{f'{itype}_accuracy': acc for itype, acc in type_acc.items()}\n",
    "    })\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pair_and_interaction_counts(merged_df):\n",
    "    TECHNIQUE_ORDER = [\n",
    "        'zero_shot_output',\n",
    "        'few_shot_output_2',\n",
    "        'few_shot_output_5',\n",
    "        'few_shot_output_10',\n",
    "        'few_shot_output_20',\n",
    "        'cot_output'\n",
    "    ]\n",
    "\n",
    "    TECHNIQUE_MAP = {\n",
    "        'zero_shot_output': 'Zero-Shot',\n",
    "        'few_shot_output_2': 'Few-Shot (2)',\n",
    "        'few_shot_output_5': 'Few-Shot (5)',\n",
    "        'few_shot_output_10': 'Few-Shot (10)',\n",
    "        'few_shot_output_20': 'Few-Shot (20)',\n",
    "        'cot_output': 'Chain-of-Thought'\n",
    "    }\n",
    "    \n",
    "    per_method_details = []\n",
    "    per_interaction_details = []\n",
    "    \n",
    "    for tech_col in TECHNIQUE_ORDER:\n",
    "        if tech_col not in merged_df.columns:\n",
    "            continue\n",
    "        \n",
    "        tech_name = TECHNIQUE_MAP.get(tech_col, tech_col)\n",
    "        \n",
    "        interaction_correct, interaction_total = 0, 0\n",
    "        per_class = {itype: {'tp': 0, 'fn': 0} for itype in INTERACTION_TYPES}\n",
    "        \n",
    "        for _, row in merged_df.iterrows():\n",
    "            # Ground truth triplets\n",
    "            gold_triplets = set()\n",
    "            if isinstance(row['triplets'], list):\n",
    "                for triplet in row['triplets']:\n",
    "                    if len(triplet) == 3:\n",
    "                        d1, itype, d2 = [x.strip().lower() for x in triplet]\n",
    "                        itype = normalize_interaction_type(itype)  # <-- FIX\n",
    "                        gold_triplets.add((normalize_drug_pair(d1, d2), itype))\n",
    "            \n",
    "            # Predictions\n",
    "            pred_triplets = set()\n",
    "            for d1, itype, d2 in parse_prediction_output(row[tech_col]):\n",
    "                itype = normalize_interaction_type(itype)  # <-- FIX\n",
    "                pred_triplets.add((normalize_drug_pair(d1, d2), itype))\n",
    "            \n",
    "            # Pair-level counts (correct if both pair+interaction match)\n",
    "            gold_pairs = {dp for dp, _ in gold_triplets}\n",
    "            for dp, it in pred_triplets:\n",
    "                if dp in gold_pairs:  # model predicted some interaction for a true pair\n",
    "                    interaction_total += 1\n",
    "                    if (dp, it) in gold_triplets:\n",
    "                        interaction_correct += 1\n",
    "            \n",
    "            # Per-interaction type counts\n",
    "            for itype in INTERACTION_TYPES:\n",
    "                gold_sub = {(dp, it) for dp, it in gold_triplets if it == itype}\n",
    "                pred_sub = {(dp, it) for dp, it in pred_triplets if it == itype}\n",
    "                \n",
    "                tp = len(gold_sub & pred_sub)\n",
    "                fn = len(gold_sub - pred_sub)\n",
    "                \n",
    "                per_class[itype]['tp'] += tp\n",
    "                per_class[itype]['fn'] += fn\n",
    "        \n",
    "        # Store per-method totals\n",
    "        per_method_details.append({\n",
    "            \"Technique\": tech_name,\n",
    "            \"Correct Pairs\": interaction_correct,\n",
    "            \"Total Pairs\": interaction_total\n",
    "        })\n",
    "        \n",
    "        # Store per-interaction type totals\n",
    "        for itype in INTERACTION_TYPES:\n",
    "            tp = per_class[itype]['tp']\n",
    "            fn = per_class[itype]['fn']\n",
    "            per_interaction_details.append({\n",
    "                \"Technique\": tech_name,\n",
    "                \"Interaction Type\": itype,\n",
    "                \"Correct\": tp,\n",
    "                \"Total\": tp + fn\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(per_method_details), pd.DataFrame(per_interaction_details)\n",
    "method_df, interaction_df = calculate_pair_and_interaction_counts(merged_df)\n",
    "print(\"PER-METHOD PAIR COUNTS:\")\n",
    "print(method_df.to_string(index=False))\n",
    "print(\"\\nPER-INTERACTION TYPE COUNTS:\")\n",
    "print(interaction_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30af9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f3c796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
