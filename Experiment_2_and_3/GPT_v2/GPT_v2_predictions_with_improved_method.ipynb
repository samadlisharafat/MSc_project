{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5269c9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80fe0277",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691a05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and processing \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "whole_df = pd.read_csv(\"C:/Users/Sharafat/Desktop/Masters' Dissertation/Structured_whole_dataframe.csv\")\n",
    "whole_df = whole_df.drop(\"Unnamed: 0\", axis = 1)\n",
    "train_df = whole_df[whole_df[\"source_x\"].astype(str).str.startswith(\"DDICorpus/Train/\")]\n",
    "train_df\n",
    "train_df[\"type\"] = train_df[\"type\"].replace(\"No interaction\", \"no interaction\")\n",
    "train_df\n",
    "test_df = whole_df[whole_df[\"source_x\"].astype(str).str.startswith(\"DDICorpus/Test/\")]\n",
    "test_df\n",
    "test_df[\"type\"] = test_df[\"type\"].replace(\"No interaction\", \"no interaction\")\n",
    "test_df\n",
    "# example => train_df or test_df \n",
    "train_df_records = train_df.to_dict(orient = \"records\")\n",
    "train_df_records\n",
    "test_df_records = test_df.to_dict(orient = \"records\")\n",
    "test_df_records\n",
    "# 1. Sample 10 examples of 'no interaction'\n",
    "no_interaction_df = train_df[train_df['type'] == 'no interaction'].sample(n=10, random_state=42)\n",
    "\n",
    "# 2. Sample 3 examples from each of the *other* types (mechanism, effect, etc.)\n",
    "other_types_df = (\n",
    "    train_df[train_df['type'] != 'no interaction']\n",
    "    .groupby('type', group_keys=False)\n",
    "    .sample(n=3, random_state=42)\n",
    ")\n",
    "\n",
    "# 3. Combine and shuffle\n",
    "examples_df = pd.concat([no_interaction_df, other_types_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "examples_df[\"type\"] = examples_df[\"type\"].replace(\"No interaction\", \"no interaction\")\n",
    "examples_df\n",
    "example_shots = examples_df.to_dict(orient=\"records\")\n",
    "example_shots\n",
    "train_df_grouped = train_df.groupby(\"sentence_id\")\n",
    "train_df_grouped\n",
    "group = train_df_grouped.get_group(\"DDI-DrugBank.d64.s87\")\n",
    "group\n",
    "train_df[\"triplet\"] = list(zip(train_df[\"e1_text\"], train_df[\"type\"], train_df[\"e2_text\"]))\n",
    "train_df\n",
    "import json\n",
    "\n",
    "# Group by sentence (assuming sentence text is unique identifier)\n",
    "grouped = train_df.groupby(\"sentence_text\")\n",
    "\n",
    "# Build JSON-like structure\n",
    "json_data = []\n",
    "\n",
    "for sentence_text, group in grouped:\n",
    "    # Get triplets from the group\n",
    "    triplets = list(zip(group[\"e1_text\"], group[\"type\"], group[\"e2_text\"]))\n",
    "    \n",
    "    # Optional: if 'type4' column exists, get the most frequent or first type4\n",
    "    type4_value = group[\"type\"].iloc[0]\n",
    "    \n",
    "    # Create record\n",
    "    json_data.append({\n",
    "        \"sentence\": sentence_text,\n",
    "        \"interaction_type\": type4_value,\n",
    "        \"triplets\": triplets\n",
    "    })\n",
    "\n",
    "# Save to file\n",
    "with open(\"train_df_sentence_triplets.json\", \"w\") as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "\n",
    "with open(\"train_df_sentence_triplets.json\",\"r\") as f:\n",
    "    train_triplets = json.load(f)\n",
    "\n",
    "train_triplets[0]\n",
    "import json\n",
    "\n",
    "# Group by sentence (assuming sentence text is unique identifier)\n",
    "grouped = test_df.groupby(\"sentence_text\")\n",
    "\n",
    "# Build JSON-like structure\n",
    "json_data = []\n",
    "\n",
    "for sentence_text, group in grouped:\n",
    "    # Get triplets from the group\n",
    "    triplets = list(zip(group[\"e1_text\"], group[\"type\"], group[\"e2_text\"]))\n",
    "    \n",
    "    # Optional: if 'type4' column exists, get the most frequent or first type4\n",
    "    type4_value = group[\"type\"].iloc[0]\n",
    "    \n",
    "    # Create record\n",
    "    json_data.append({\n",
    "        \"sentence\": sentence_text,\n",
    "        \"interaction_type\": type4_value,\n",
    "        \"triplets\": triplets\n",
    "    })\n",
    "\n",
    "# Save to file\n",
    "with open(\"test_df_sentence_triplets.json\", \"w\") as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "with open(\"test_df_sentence_triplets.json\",\"r\") as f:\n",
    "    test_triplets = json.load(f)\n",
    "\n",
    "test_triplets[0]\n",
    "import json\n",
    "\n",
    "# Group by unique sentence (or sentence_id if preferred)\n",
    "grouped = train_df.groupby(\"sentence_text\")\n",
    "\n",
    "# Build JSON-like structure\n",
    "sentence_entities = []\n",
    "\n",
    "for sentence_text, group in grouped:\n",
    "    # Collect unique entities from both drug1 and drug2\n",
    "    entities = set(group[\"e1_text\"]).union(set(group[\"e2_text\"]))\n",
    "    \n",
    "    sentence_entities.append({\n",
    "        \"sentence\": sentence_text,\n",
    "        \"entities\": sorted(entities)  # optional: sort for consistency\n",
    "    })\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"train_df_sentence_entities.json\", \"w\") as f:\n",
    "    json.dump(sentence_entities, f, indent=2)\n",
    "\n",
    "with open(\"train_df_sentence_entities.json\",\"r\") as f:\n",
    "    train_entities = json.load(f)\n",
    "\n",
    "train_entities[0]\n",
    "train_triplets\n",
    "few_shot_examples = train_triplets\n",
    "few_shot_examples\n",
    "import json\n",
    "\n",
    "# Group by unique sentence (or sentence_id if preferred)\n",
    "grouped = test_df.groupby(\"sentence_text\")\n",
    "\n",
    "# Build JSON-like structure\n",
    "sentence_entities = []\n",
    "\n",
    "for sentence_text, group in grouped:\n",
    "    # Collect unique entities from both drug1 and drug2\n",
    "    entities = set(group[\"e1_text\"]).union(set(group[\"e2_text\"]))\n",
    "    \n",
    "    sentence_entities.append({\n",
    "        \"sentence\": sentence_text,\n",
    "        \"entities\": sorted(entities)  # optional: sort for consistency\n",
    "    })\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"test_df_sentence_entities.json\", \"w\") as f:\n",
    "    json.dump(sentence_entities, f, indent=2)\n",
    "\n",
    "with open(\"test_df_sentence_entities.json\",\"r\") as f:\n",
    "    test_entities = json.load(f)\n",
    "\n",
    "test_entities[0]\n",
    "len(train_entities)\n",
    "len(test_entities)\n",
    "examples_df\n",
    "import json\n",
    "\n",
    "# Group by sentence (assuming sentence text is unique identifier)\n",
    "grouped = examples_df.groupby(\"sentence_text\")\n",
    "\n",
    "# Build JSON-like structure\n",
    "json_data = []\n",
    "\n",
    "for sentence_text, group in grouped:\n",
    "    # Get triplets from the group\n",
    "    triplets = list(zip(group[\"e1_text\"], group[\"type\"], group[\"e2_text\"]))\n",
    "    \n",
    "    # Optional: if 'type4' column exists, get the most frequent or first type4\n",
    "    type4_value = group[\"type\"].iloc[0]\n",
    "    \n",
    "    # Create record\n",
    "    json_data.append({\n",
    "        \"sentence\": sentence_text,\n",
    "        \"interaction_type\": type4_value,\n",
    "        \"triplets\": triplets\n",
    "    })\n",
    "\n",
    "# Save to file\n",
    "with open(\"few_shot_example_triplets.json\", \"w\") as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "\n",
    "with open(\"few_shot_example_triplets.json\",\"r\") as f:\n",
    "    few_shot_entities = json.load(f)\n",
    "\n",
    "few_shot_entities[0]\n",
    "len(few_shot_entities)\n",
    "train_entities\n",
    "train_triplets # ground truth, remove interaction_type and choose random examples for few_shot\n",
    "def transform_structure(data):\n",
    "    output = {'sentence': data['sentence']}\n",
    "    for idx, entity in enumerate(data['entities'], 1):\n",
    "        output[f'Entity_{idx}'] = entity\n",
    "    return output\n",
    "\n",
    "transformed = transform_structure(train_entities[0])\n",
    "print(transformed)\n",
    "\n",
    "def transform_structure(data):\n",
    "    output = {'Sentence': data['sentence']}\n",
    "    for idx, entity in enumerate(data['entities'], 1):\n",
    "        output[f'Entity_{idx}'] = entity\n",
    "    return output\n",
    "\n",
    "updated_train_entities = []\n",
    "for i in range(len(train_entities)):\n",
    "    transformed = transform_structure(train_entities[i])\n",
    "    updated_train_entities.append(transformed)\n",
    "print(updated_train_entities[-1])\n",
    "len(updated_train_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b129afa3",
   "metadata": {},
   "source": [
    "# GPT v2 improved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c66bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"random_few_shot_examples_2.json\",\"r\") as f:\n",
    "    few_shot_prompt_2_examples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4eab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"random_few_shot_examples_5.json\",\"r\") as f:\n",
    "    few_shot_prompt_5_examples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b177d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"random_few_shot_examples_10.json\",\"r\") as f:\n",
    "    few_shot_prompt_10_examples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b60ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"random_few_shot_examples_20.json\",\"r\") as f:\n",
    "    few_shot_prompt_20_examples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101390b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from google import genai\n",
    "\n",
    "\n",
    "GEMINI_API_KEY = ''\n",
    "assert GEMINI_API_KEY, \"\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
    "client = genai.Client()\n",
    "\n",
    "# Output holder\n",
    "all_responses = []\n",
    "\n",
    "# Prompt styles\n",
    "interaction_types = \"\"\"\n",
    "- mechanism: Pharmacokinetic interaction (metabolism, clearance, absorption, transport). Cues: inhibits/induces, substrate, CYP/UGT/P-gp, AUC/Cmax change, plasma levels ↑/↓, bioavailability, clearance.\n",
    "- effect: Pharmacodynamic clinical effect or outcome. Cues: bleeding, sedation, QT prolongation, hypotension, serotonin syndrome, reduced efficacy, toxicity, adverse events.\n",
    "- advice: Explicit recommendation/warning/contraindication. Cues: avoid, do not use, contraindicated, use with caution, monitor, reduce/increase dose, separate by X hours.\n",
    "- int: A generic mention that an interaction exists without mechanism, effect, or advice. Cues: “interacts”, “interaction exists”, “DDI reported”, “potential interaction” with no further details.\n",
    "- no interaction: Explicit statement of no interaction or safe coadministration (e.g., “no significant interaction”, “can be coadministered”, “safe with”).\n",
    "\"\"\"\n",
    "\n",
    "rules = \"\"\" Rules (strict):\n",
    "A. Allowed labels (lowercase) are exactly: mechanism, effect, advice, int, no interaction.\n",
    "   - Never output \"advise\". If your reasoning suggests \"advise\", output the label \"advice\".\n",
    "B. One triplet per unique unordered pair of entities. Do not output both (Drug1, x, Drug2) and (Drug2, x, Drug1).\n",
    "C. Decision hierarchy (to avoid misuse of \"int\"):\n",
    "   1) If any recommendation/warning/contraindication language appears → advice.\n",
    "   2) Else if PK level/process is stated (inhibit/induce/CYP/levels/AUC/clearance/absorption/transport) → mechanism.\n",
    "   3) Else if clinical effect/outcome/AE/risk is stated (bleeding, sedation, etc.) → effect.\n",
    "   4) Else if the text only states there is an interaction with no details → int.\n",
    "   5) Else → no interaction.\n",
    "D. “Monitor”, “use with caution”, “dose adjust”, “separate dosing” are **advice**, not int.\n",
    "E. “Levels/concentrations/AUC/Cmax change”, “CYP/UGT/P-gp”, “inhibits/induces/substrate” are **mechanism**, not int.\n",
    "F. “Increased risk of [event]” or any clinical outcome is **effect**, not int.\n",
    "G. Output only triplets in the form: (Drug1, label, Drug2). No extra text. \"\"\"\n",
    "\n",
    "few_shot_rules = \"\"\" Rules (strict):\n",
    "A. Allowed labels (lowercase) are exactly: mechanism, effect, advice, int, no interaction.\n",
    "   - Never output \"advise\". If your reasoning suggests \"advise\", output the label \"advice\".\n",
    "B. One triplet per unique unordered pair of entities. Do not output both (Drug1, x, Drug2) and (Drug2, x, Drug1).\n",
    "C. Decision hierarchy (to avoid misuse of \"int\"):\n",
    "   1) advice > 2) mechanism > 3) effect > 4) int > 5) no interaction.\n",
    "D. “Monitor”, “use with caution”, “dose adjust”, “separate dosing” → advice.\n",
    "E. PK cues (CYP/UGT/P-gp, levels/AUC/Cmax, inhibit/induce/substrate) → mechanism.\n",
    "F. Clinical outcome/risk terms → effect.\n",
    "G. Output only triplets in the exact form: (Drug1, label, Drug2). \"\"\"\n",
    "\n",
    "few_shot_examples = f\"\"\"\n",
    "{interaction_types}\n",
    "\"\"\"\n",
    "\n",
    "def extract_entities(entity_dict):\n",
    "    return [v for k, v in entity_dict.items() if k.startswith(\"Entity_\")]\n",
    "\n",
    "# Inference loop\n",
    "for i in range(0, len(updated_train_entities)):\n",
    "    example = updated_train_entities[i]\n",
    "    sentence = example[\"Sentence\"]\n",
    "    entities = extract_entities(example)\n",
    "\n",
    "    output_entry = {\n",
    "        \"index\": i,\n",
    "        \"sentence\": sentence,\n",
    "        \"entities\": entities\n",
    "    }\n",
    "\n",
    "    # Prepare prompts\n",
    "    zero_shot_prompt = f\"\"\"You are a biomedical expert. Extract drug-drug interactions (DDIs) from the following text.\n",
    "\n",
    "Definitions of interaction types: {interaction_types}\n",
    "Rules are given: {rules}\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\"\"\"\n",
    "\n",
    "    few_shot_prompt_2 = f\"\"\"You are a biomedical expert. Extract drug-drug interactions (DDIs) from biomedical text.\n",
    "\n",
    " Definitions of interaction types: {interaction_types}\n",
    " Rules are given: {few_shot_rules}\n",
    "\n",
    "Examples: {few_shot_prompt_2_examples}\n",
    "\n",
    "Now extract triplets for:\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\"\"\"\n",
    "    \n",
    "    few_shot_prompt_5 = f\"\"\"You are a biomedical expert. Extract drug-drug interactions (DDIs) from biomedical text.\n",
    "\n",
    "Interaction types:{interaction_types}\n",
    "Rules are given: {few_shot_rules}\n",
    "\n",
    "Examples: {few_shot_prompt_5_examples}\n",
    "\n",
    "Now extract triplets for:\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\"\"\"\n",
    "\n",
    "\n",
    "    few_shot_prompt_10 = f\"\"\"You are a biomedical expert. Extract drug-drug interactions (DDIs) from biomedical text.\n",
    "\n",
    "Interaction types: {interaction_types}\n",
    "Rules are given: {few_shot_rules}\n",
    "\n",
    "Examples: {few_shot_prompt_10_examples}\n",
    "\n",
    "Now extract triplets for:\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\"\"\"\n",
    "    \n",
    "    few_shot_prompt_20 = f\"\"\"You are a biomedical expert. Extract drug-drug interactions (DDIs) from biomedical text.\n",
    "\n",
    "Interaction types: {interaction_types}\n",
    "Rules are given: {few_shot_rules}\n",
    "\n",
    "\n",
    "Examples: {few_shot_prompt_20_examples}\n",
    "\n",
    "Now extract triplets for:\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\"\"\"\n",
    "\n",
    "\n",
    "    cot_prompt = f\"\"\"You are a biomedical expert. Perform the following:\n",
    "\n",
    "1) List unique unordered entity pairs.\n",
    "2) For each pair, locate evidence span(s) in the sentence.\n",
    "3) Apply the decision hierarchy strictly (advice > mechanism > effect > int > no interaction) considering definition of labels: {interaction_types}.\n",
    "4) Canonicalize the label set to: mechanism, effect, advice, int, no interaction.\n",
    "   - If you would output \"advise\", output \"advice\" instead.\n",
    "5) Produce exactly one triplet per pair.\n",
    "\n",
    "Rules are given: {rules}\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\n",
    "\"\"\"\n",
    "\n",
    "    prompts = {\n",
    "        \"zero_shot_output\": zero_shot_prompt,\n",
    "        \"few_shot_output_2\": few_shot_prompt_2,\n",
    "        \"few_shot_output_5\": few_shot_prompt_5,\n",
    "        \"few_shot_output_10\": few_shot_prompt_10,\n",
    "        \"few_shot_output_20\": few_shot_prompt_20,\n",
    "        \"cot_output\": cot_prompt\n",
    "    }\n",
    "\n",
    "    for method_name, prompt in prompts.items():\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemma-3-27b-it\",\n",
    "                contents=prompt,\n",
    "                config=genai.types.GenerateContentConfig(\n",
    "                    temperature=0.0\n",
    "                )\n",
    "            )\n",
    "            time.sleep(6)\n",
    "\n",
    "            output_entry[method_name] = response.text.strip()\n",
    "            print(f\"[{method_name}] Finished index {i}\")\n",
    "            print(f\"{method_name}: {response.text.strip()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {i} with method {method_name}: {e}\")\n",
    "            output_entry[method_name] = \"ERROR\"\n",
    "\n",
    "    all_responses.append(output_entry)\n",
    "\n",
    "    # Save after each example (optional safety)\n",
    "    with open(\"GPT_Prompting_method_2.json\", \"w\") as f:\n",
    "        json.dump(all_responses, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeffd57",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952a7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open(\"GPT_Prompting_method_2_last.json\", \"r\") as f:\n",
    "    all_responses = json.load(f)\n",
    "all_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657955e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplets_df = pd.DataFrame(train_triplets)\n",
    "train_triplets_df\n",
    "triplet_prediction_results = pd.DataFrame(all_responses)\n",
    "triplet_prediction_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a1ff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = train_triplets_df.merge(triplet_prediction_results)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fdc1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "# Configuration\n",
    "INTERACTION_TYPES = {\"effect\", \"mechanism\", \"advice\", \"int\", \"no interaction\"}\n",
    "MODEL_MAP = {\n",
    "    'zero_shot_output': 'Zero-Shot',\n",
    "    'few_shot_output_2': 'Few-Shot-2',\n",
    "    'few_shot_output_5': 'Few-Shot-5',\n",
    "    'few_shot_output_10': 'Few-Shot-10',\n",
    "    'few_shot_output_20': 'Few-Shot-20',\n",
    "    'cot_output': 'CoT'\n",
    "}\n",
    "MODEL_ORDER = list(MODEL_MAP.keys())\n",
    "\n",
    "# --- Normalization ---\n",
    "def normalize_interaction_type(itype: str) -> str:\n",
    "    \"\"\"Map variations of interaction types to canonical form\"\"\"\n",
    "    itype = itype.strip().lower()\n",
    "    if itype in {\"advise\", \"adviced\", \"advised\", \"advice\"}:\n",
    "        return \"advice\"\n",
    "    return itype\n",
    "\n",
    "def normalize_drug_pair(drug1, drug2):\n",
    "    \"\"\"Create canonical drug pair representation\"\"\"\n",
    "    return tuple(sorted([drug1.strip().lower(), drug2.strip().lower()]))\n",
    "\n",
    "# --- Parsing ---\n",
    "def parse_prediction_output(pred_output):\n",
    "    \"\"\"Robust parser that requires exactly 2 drugs and 1 interaction type\"\"\"\n",
    "    if not isinstance(pred_output, str):\n",
    "        return []\n",
    "    \n",
    "    triplet_pattern = r'\\(([^,]+?),\\s*([^,]+?),\\s*([^)]+?)\\)'\n",
    "    raw_triplets = re.findall(triplet_pattern, pred_output)\n",
    "    \n",
    "    cleaned_triplets = []\n",
    "    for triplet in raw_triplets:\n",
    "        parts = [p.strip().lower() for p in triplet]\n",
    "        norm_parts = [normalize_interaction_type(p) for p in parts]\n",
    "        \n",
    "        # Identify interaction type candidates\n",
    "        interaction_candidates = [p for p in norm_parts if p in INTERACTION_TYPES]\n",
    "        drug_candidates = [parts[i] for i, p in enumerate(norm_parts) if p not in INTERACTION_TYPES]\n",
    "        \n",
    "        if len(interaction_candidates) == 1 and len(drug_candidates) == 2:\n",
    "            itype = interaction_candidates[0]\n",
    "            cleaned_triplets.append((drug_candidates[0], itype, drug_candidates[1]))\n",
    "            continue\n",
    "        \n",
    "        # Fallback: positional checks\n",
    "        if len(parts) == 3:\n",
    "            if norm_parts[1] in INTERACTION_TYPES:\n",
    "                cleaned_triplets.append((parts[0], norm_parts[1], parts[2]))\n",
    "            elif norm_parts[0] in INTERACTION_TYPES:\n",
    "                cleaned_triplets.append((parts[1], norm_parts[0], parts[2]))\n",
    "            elif norm_parts[2] in INTERACTION_TYPES:\n",
    "                cleaned_triplets.append((parts[0], norm_parts[2], parts[1]))\n",
    "    \n",
    "    return cleaned_triplets\n",
    "\n",
    "\n",
    "# --- Evaluation ---\n",
    "def evaluate_predictions(df):\n",
    "    \"\"\"Comprehensive evaluation with model comparison metrics\"\"\"\n",
    "    results = []\n",
    "    per_class_results = []\n",
    "    \n",
    "    for model_col in MODEL_ORDER:\n",
    "        if model_col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        model_name = MODEL_MAP[model_col]\n",
    "        \n",
    "        # Initialize metrics\n",
    "        metrics = {\n",
    "            'Model': model_name,\n",
    "            'Accuracy': 0,\n",
    "            'Micro_Precision': 0, 'Micro_Recall': 0, 'Micro_F1': 0,\n",
    "            'Macro_Precision': 0, 'Macro_Recall': 0, 'Macro_F1': 0,\n",
    "            'Weighted_Precision': 0, 'Weighted_Recall': 0, 'Weighted_F1': 0,\n",
    "        }\n",
    "        \n",
    "        class_metrics = {itype: {'TP': 0, 'FP': 0, 'FN': 0, 'TN': 0} for itype in INTERACTION_TYPES}\n",
    "        all_true = []\n",
    "        all_pred = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Process ground truth\n",
    "            gold_triplets = set()\n",
    "            for triplet in row['triplets']:\n",
    "                if len(triplet) == 3:\n",
    "                    d1, itype, d2 = [x.strip().lower() for x in triplet]\n",
    "                    itype = normalize_interaction_type(itype)\n",
    "                    gold_triplets.add((normalize_drug_pair(d1, d2), itype))\n",
    "            \n",
    "            # Process predictions\n",
    "            pred_triplets = set()\n",
    "            for d1, itype, d2 in parse_prediction_output(row[model_col]):\n",
    "                itype = normalize_interaction_type(itype)\n",
    "                pred_triplets.add((normalize_drug_pair(d1, d2), itype))\n",
    "            \n",
    "            # Create lists for sklearn metrics\n",
    "            for itype in INTERACTION_TYPES:\n",
    "                # True positive: both gold and prediction have this interaction\n",
    "                if any((dp, t) in gold_triplets and t == itype for dp, t in pred_triplets):\n",
    "                    class_metrics[itype]['TP'] += 1\n",
    "                    all_true.append(itype)\n",
    "                    all_pred.append(itype)\n",
    "                # False positive: prediction has this interaction but gold doesn't\n",
    "                elif any(t == itype for dp, t in pred_triplets):\n",
    "                    class_metrics[itype]['FP'] += 1\n",
    "                    all_pred.append(itype)\n",
    "                    # Assign the true label\n",
    "                    true_label = \"no interaction\"\n",
    "                    for dp, t in gold_triplets:\n",
    "                        if dp in [dp_p for dp_p, t_p in pred_triplets if t_p == itype]:\n",
    "                            true_label = t\n",
    "                            break\n",
    "                    all_true.append(true_label)\n",
    "                # False negative: gold has this interaction but prediction doesn't\n",
    "                elif any(t == itype for dp, t in gold_triplets):\n",
    "                    class_metrics[itype]['FN'] += 1\n",
    "                    all_true.append(itype)\n",
    "                    all_pred.append(\"no interaction\")  # Missed prediction\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct = sum(1 for t, p in zip(all_true, all_pred) if t == p)\n",
    "        total = len(all_true) if all_true else 1\n",
    "        metrics['Accuracy'] = correct / total if total > 0 else 0\n",
    "        \n",
    "        # Calculate micro, macro, and weighted metrics using sklearn\n",
    "        if all_true and all_pred:\n",
    "            micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(\n",
    "                all_true, all_pred, average='micro', zero_division=0)\n",
    "            macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
    "                all_true, all_pred, average='macro', zero_division=0)\n",
    "            weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
    "                all_true, all_pred, average='weighted', zero_division=0)\n",
    "            \n",
    "            metrics.update({\n",
    "                'Micro_Precision': micro_precision,\n",
    "                'Micro_Recall': micro_recall,\n",
    "                'Micro_F1': micro_f1,\n",
    "                'Macro_Precision': macro_precision,\n",
    "                'Macro_Recall': macro_recall,\n",
    "                'Macro_F1': macro_f1,\n",
    "                'Weighted_Precision': weighted_precision,\n",
    "                'Weighted_Recall': weighted_recall,\n",
    "                'Weighted_F1': weighted_f1\n",
    "            })\n",
    "        \n",
    "        results.append(metrics)\n",
    "        \n",
    "        # Calculate per-class metrics\n",
    "        for itype in INTERACTION_TYPES:\n",
    "            cm = class_metrics[itype]\n",
    "            p = cm['TP'] / (cm['TP'] + cm['FP']) if cm['TP'] + cm['FP'] > 0 else 0\n",
    "            r = cm['TP'] / (cm['TP'] + cm['FN']) if cm['TP'] + cm['FN'] > 0 else 0\n",
    "            f = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "            support = cm['TP'] + cm['FN']\n",
    "            \n",
    "            per_class_results.append({\n",
    "                'Model': model_name,\n",
    "                'Interaction': itype,\n",
    "                'Precision': p,\n",
    "                'Recall': r,\n",
    "                'F1': f,\n",
    "                'Support': support\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results), pd.DataFrame(per_class_results)\n",
    "\n",
    "# --- Report ---\n",
    "def generate_comparison_report(global_df, class_df):\n",
    "    \"\"\"Create model comparison report with sorting\"\"\"\n",
    "    global_df = global_df.sort_values('Micro_F1', ascending=False)\n",
    "    \n",
    "    # Format numbers\n",
    "    global_df = global_df.round(4)\n",
    "    class_df = class_df.round(4)\n",
    "    \n",
    "    report = \"MODEL COMPARISON REPORT\\n\"\n",
    "    report += \"=======================\\n\\n\"\n",
    "    report += \"Global Metrics:\\n\"\n",
    "    report += global_df.to_string(index=False) + \"\\n\\n\"\n",
    "    report += \"Per-Class Metrics:\\n\"\n",
    "    report += class_df.to_string(index=False)\n",
    "    \n",
    "    best_model = global_df.iloc[0]\n",
    "    report += f\"\\n\\nBest Model: {best_model['Model']} (Macro F1={best_model['Macro_F1']:.4f}, Micro F1={best_model['Micro_F1']:.4f})\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    global_results, class_results = evaluate_predictions(merged_df)\n",
    "    report = generate_comparison_report(global_results, class_results)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfed56c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pair_and_interaction_counts(merged_df):\n",
    "    TECHNIQUE_ORDER = [\n",
    "        'zero_shot_output',\n",
    "        'few_shot_output_2',\n",
    "        'few_shot_output_5',\n",
    "        'few_shot_output_10',\n",
    "        'few_shot_output_20',\n",
    "        'cot_output'\n",
    "    ]\n",
    "\n",
    "    TECHNIQUE_MAP = {\n",
    "        'zero_shot_output': 'Zero-Shot',\n",
    "        'few_shot_output_2': 'Few-Shot (2)',\n",
    "        'few_shot_output_5': 'Few-Shot (5)',\n",
    "        'few_shot_output_10': 'Few-Shot (10)',\n",
    "        'few_shot_output_20': 'Few-Shot (20)',\n",
    "        'cot_output': 'Chain-of-Thought'\n",
    "    }\n",
    "    \n",
    "    per_method_details = []\n",
    "    per_interaction_details = []\n",
    "    \n",
    "    for tech_col in TECHNIQUE_ORDER:\n",
    "        if tech_col not in merged_df.columns:\n",
    "            continue\n",
    "        \n",
    "        tech_name = TECHNIQUE_MAP.get(tech_col, tech_col)\n",
    "        \n",
    "        interaction_correct, interaction_total = 0, 0\n",
    "        per_class = {itype: {'tp': 0, 'fn': 0} for itype in INTERACTION_TYPES}\n",
    "        \n",
    "        for _, row in merged_df.iterrows():\n",
    "            # Ground truth triplets\n",
    "            gold_triplets = set()\n",
    "            if isinstance(row['triplets'], list):\n",
    "                for triplet in row['triplets']:\n",
    "                    if len(triplet) == 3:\n",
    "                        d1, itype, d2 = [x.strip().lower() for x in triplet]\n",
    "                        itype = normalize_interaction_type(itype)  # <-- FIX\n",
    "                        gold_triplets.add((normalize_drug_pair(d1, d2), itype))\n",
    "            \n",
    "            # Predictions\n",
    "            pred_triplets = set()\n",
    "            for d1, itype, d2 in parse_prediction_output(row[tech_col]):\n",
    "                itype = normalize_interaction_type(itype)  # <-- FIX\n",
    "                pred_triplets.add((normalize_drug_pair(d1, d2), itype))\n",
    "            \n",
    "            # Pair-level counts (correct if both pair+interaction match)\n",
    "            gold_pairs = {dp for dp, _ in gold_triplets}\n",
    "            for dp, it in pred_triplets:\n",
    "                if dp in gold_pairs:  # model predicted some interaction for a true pair\n",
    "                    interaction_total += 1\n",
    "                    if (dp, it) in gold_triplets:\n",
    "                        interaction_correct += 1\n",
    "            \n",
    "            # Per-interaction type counts\n",
    "            for itype in INTERACTION_TYPES:\n",
    "                gold_sub = {(dp, it) for dp, it in gold_triplets if it == itype}\n",
    "                pred_sub = {(dp, it) for dp, it in pred_triplets if it == itype}\n",
    "                \n",
    "                tp = len(gold_sub & pred_sub)\n",
    "                fn = len(gold_sub - pred_sub)\n",
    "                \n",
    "                per_class[itype]['tp'] += tp\n",
    "                per_class[itype]['fn'] += fn\n",
    "        \n",
    "        # Store per-method totals\n",
    "        per_method_details.append({\n",
    "            \"Technique\": tech_name,\n",
    "            \"Correct Pairs\": interaction_correct,\n",
    "            \"Total Pairs\": interaction_total\n",
    "        })\n",
    "        \n",
    "        # Store per-interaction type totals\n",
    "        for itype in INTERACTION_TYPES:\n",
    "            tp = per_class[itype]['tp']\n",
    "            fn = per_class[itype]['fn']\n",
    "            per_interaction_details.append({\n",
    "                \"Technique\": tech_name,\n",
    "                \"Interaction Type\": itype,\n",
    "                \"Correct\": tp,\n",
    "                \"Total\": tp + fn\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(per_method_details), pd.DataFrame(per_interaction_details)\n",
    "method_df, interaction_df = calculate_pair_and_interaction_counts(merged_df)\n",
    "print(\"PER-METHOD PAIR COUNTS:\")\n",
    "print(method_df.to_string(index=False))\n",
    "print(\"\\nPER-INTERACTION TYPE COUNTS:\")\n",
    "print(interaction_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a4cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_class_distributions(class_df):\n",
    "    \"\"\"\n",
    "    Visualize correct prediction distribution per class across models.\n",
    "    Each bar shows recall (correct / total).\n",
    "    \"\"\"\n",
    "    classes = sorted(class_df['Interaction'].unique())\n",
    "    models = class_df['Model'].unique()\n",
    "\n",
    "    # Make one subplot per interaction type\n",
    "    fig, axes = plt.subplots(len(classes), 1, figsize=(10, 4 * len(classes)), sharex=True)\n",
    "\n",
    "    if len(classes) == 1:\n",
    "        axes = [axes]  # Ensure iterable\n",
    "\n",
    "    for ax, itype in zip(axes, classes):\n",
    "        subset = class_df[class_df['Interaction'] == itype]\n",
    "\n",
    "        recalls = subset.set_index('Model')['Recall']\n",
    "        supports = subset.set_index('Model')['Support']\n",
    "\n",
    "        # Plot recall bars\n",
    "        recalls.plot(kind='bar', ax=ax, color='purple', edgecolor='black')\n",
    "\n",
    "        # Annotate bars with \"TP / Support\"\n",
    "        for i, model in enumerate(recalls.index):\n",
    "            tp = int(round(recalls[model] * supports[model]))\n",
    "            ax.text(i, recalls[model] + 0.02, f\"{tp}/{supports[model]}\", \n",
    "                    ha='center', fontsize=9, rotation=0)\n",
    "\n",
    "        ax.set_ylim(0, 1.05)\n",
    "        ax.set_title(f\"Class: {itype}\", fontsize=14)\n",
    "        ax.set_ylabel(\"Recall (Correct / Total)\")\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    plt.suptitle(\"Per-Class Prediction Distribution per Model\", fontsize=16, y=1.02)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0decec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    global_results, class_results = evaluate_predictions(merged_df)\n",
    "    report = generate_comparison_report(global_results, class_results)\n",
    "    print(report)\n",
    "\n",
    "    # 👇 Add this line to display the plots\n",
    "    plot_class_distributions(class_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3c5272",
   "metadata": {},
   "source": [
    "# GPT v2 model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ca48f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_structure(data):\n",
    "    output = {'sentence': data['sentence']}\n",
    "    for idx, entity in enumerate(data['entities'], 1):\n",
    "        output[f'Entity_{idx}'] = entity\n",
    "    return output\n",
    "\n",
    "transformed = transform_structure(test_entities[0])\n",
    "print(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832c9e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_structure(data):\n",
    "    output = {'Sentence': data['sentence']}\n",
    "    for idx, entity in enumerate(data['entities'], 1):\n",
    "        output[f'Entity_{idx}'] = entity\n",
    "    return output\n",
    "\n",
    "updated_test_entities = []\n",
    "for i in range(len(test_entities)):\n",
    "    transformed = transform_structure(test_entities[i])\n",
    "    updated_test_entities.append(transformed)\n",
    "print(updated_test_entities[-1])\n",
    "len(updated_test_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92adeeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from google import genai\n",
    "\n",
    "\n",
    "GEMINI_API_KEY = ''\n",
    "assert GEMINI_API_KEY, \"\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
    "client = genai.Client()\n",
    "\n",
    "# Output holder\n",
    "all_responses = []\n",
    "\n",
    "# Prompt styles\n",
    "interaction_types = \"\"\"\n",
    "- mechanism: Pharmacokinetic interaction (metabolism, clearance, absorption, transport). Cues: inhibits/induces, substrate, CYP/UGT/P-gp, AUC/Cmax change, plasma levels ↑/↓, bioavailability, clearance.\n",
    "- effect: Pharmacodynamic clinical effect or outcome. Cues: bleeding, sedation, QT prolongation, hypotension, serotonin syndrome, reduced efficacy, toxicity, adverse events.\n",
    "- advice: Explicit recommendation/warning/contraindication. Cues: avoid, do not use, contraindicated, use with caution, monitor, reduce/increase dose, separate by X hours.\n",
    "- int: A generic mention that an interaction exists without mechanism, effect, or advice. Cues: “interacts”, “interaction exists”, “DDI reported”, “potential interaction” with no further details.\n",
    "- no interaction: Explicit statement of no interaction or safe coadministration (e.g., “no significant interaction”, “can be coadministered”, “safe with”).\n",
    "\"\"\"\n",
    "\n",
    "rules = \"\"\" Rules (strict):\n",
    "A. Allowed labels (lowercase) are exactly: mechanism, effect, advice, int, no interaction.\n",
    "   - Never output \"advise\". If your reasoning suggests \"advise\", output the label \"advice\".\n",
    "B. One triplet per unique unordered pair of entities. Do not output both (Drug1, x, Drug2) and (Drug2, x, Drug1).\n",
    "C. Decision hierarchy (to avoid misuse of \"int\"):\n",
    "   1) If any recommendation/warning/contraindication language appears → advice.\n",
    "   2) Else if PK level/process is stated (inhibit/induce/CYP/levels/AUC/clearance/absorption/transport) → mechanism.\n",
    "   3) Else if clinical effect/outcome/AE/risk is stated (bleeding, sedation, etc.) → effect.\n",
    "   4) Else if the text only states there is an interaction with no details → int.\n",
    "   5) Else → no interaction.\n",
    "D. “Monitor”, “use with caution”, “dose adjust”, “separate dosing” are **advice**, not int.\n",
    "E. “Levels/concentrations/AUC/Cmax change”, “CYP/UGT/P-gp”, “inhibits/induces/substrate” are **mechanism**, not int.\n",
    "F. “Increased risk of [event]” or any clinical outcome is **effect**, not int.\n",
    "G. Output only triplets in the form: (Drug1, label, Drug2). No extra text. \"\"\"\n",
    "\n",
    "few_shot_rules = \"\"\" Rules (strict):\n",
    "A. Allowed labels (lowercase) are exactly: mechanism, effect, advice, int, no interaction.\n",
    "   - Never output \"advise\". If your reasoning suggests \"advise\", output the label \"advice\".\n",
    "B. One triplet per unique unordered pair of entities. Do not output both (Drug1, x, Drug2) and (Drug2, x, Drug1).\n",
    "C. Decision hierarchy (to avoid misuse of \"int\"):\n",
    "   1) advice > 2) mechanism > 3) effect > 4) int > 5) no interaction.\n",
    "D. “Monitor”, “use with caution”, “dose adjust”, “separate dosing” → advice.\n",
    "E. PK cues (CYP/UGT/P-gp, levels/AUC/Cmax, inhibit/induce/substrate) → mechanism.\n",
    "F. Clinical outcome/risk terms → effect.\n",
    "G. Output only triplets in the exact form: (Drug1, label, Drug2). \"\"\"\n",
    "\n",
    "few_shot_examples = f\"\"\"\n",
    "{interaction_types}\n",
    "\"\"\"\n",
    "\n",
    "def extract_entities(entity_dict):\n",
    "    return [v for k, v in entity_dict.items() if k.startswith(\"Entity_\")]\n",
    "\n",
    "# Inference loop\n",
    "for i in range(0, len(updated_test_entities)):\n",
    "    example = updated_test_entities[i]\n",
    "    sentence = example[\"Sentence\"]\n",
    "    entities = extract_entities(example)\n",
    "\n",
    "    output_entry = {\n",
    "        \"index\": i,\n",
    "        \"sentence\": sentence,\n",
    "        \"entities\": entities\n",
    "    }\n",
    "\n",
    "    # Prepare prompts\n",
    "    zero_shot_prompt = f\"\"\"You are a biomedical expert. Extract drug-drug interactions (DDIs) from the following text.\n",
    "\n",
    "Definitions of interaction types: {interaction_types}\n",
    "Rules are given: {rules}\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\"\"\"\n",
    "\n",
    "    few_shot_prompt_2 = f\"\"\"You are a biomedical expert. Extract drug-drug interactions (DDIs) from biomedical text.\n",
    "\n",
    " Definitions of interaction types: {interaction_types}\n",
    " Rules are given: {few_shot_rules}\n",
    "\n",
    "Examples: {few_shot_prompt_2_examples}\n",
    "\n",
    "Now extract triplets for:\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\"\"\"\n",
    "    \n",
    "    few_shot_prompt_5 = f\"\"\"You are a biomedical expert. Extract drug-drug interactions (DDIs) from biomedical text.\n",
    "\n",
    "Interaction types:{interaction_types}\n",
    "Rules are given: {few_shot_rules}\n",
    "\n",
    "Examples: {few_shot_prompt_5_examples}\n",
    "\n",
    "Now extract triplets for:\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\"\"\"\n",
    "\n",
    "\n",
    "    few_shot_prompt_10 = f\"\"\"You are a biomedical expert. Extract drug-drug interactions (DDIs) from biomedical text.\n",
    "\n",
    "Interaction types: {interaction_types}\n",
    "Rules are given: {few_shot_rules}\n",
    "\n",
    "Examples: {few_shot_prompt_10_examples}\n",
    "\n",
    "Now extract triplets for:\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\"\"\"\n",
    "    \n",
    "    few_shot_prompt_20 = f\"\"\"You are a biomedical expert. Extract drug-drug interactions (DDIs) from biomedical text.\n",
    "\n",
    "Interaction types: {interaction_types}\n",
    "Rules are given: {few_shot_rules}\n",
    "\n",
    "\n",
    "Examples: {few_shot_prompt_20_examples}\n",
    "\n",
    "Now extract triplets for:\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\"\"\"\n",
    "\n",
    "\n",
    "    cot_prompt = f\"\"\"You are a biomedical expert. Perform the following:\n",
    "\n",
    "1) List unique unordered entity pairs.\n",
    "2) For each pair, locate evidence span(s) in the sentence.\n",
    "3) Apply the decision hierarchy strictly (advice > mechanism > effect > int > no interaction) considering definition of labels: {interaction_types}.\n",
    "4) Canonicalize the label set to: mechanism, effect, advice, int, no interaction.\n",
    "   - If you would output \"advise\", output \"advice\" instead.\n",
    "5) Produce exactly one triplet per pair.\n",
    "\n",
    "Rules are given: {rules}\n",
    "\n",
    "Text: \"{sentence}\"\n",
    "Entities: {\", \".join(entities)}\n",
    "\n",
    "Triplets:\n",
    "\"\"\"\n",
    "\n",
    "    prompts = {\n",
    "        \"zero_shot_output\": zero_shot_prompt,\n",
    "        \"few_shot_output_2\": few_shot_prompt_2,\n",
    "        \"few_shot_output_5\": few_shot_prompt_5,\n",
    "        \"few_shot_output_10\": few_shot_prompt_10,\n",
    "        \"few_shot_output_20\": few_shot_prompt_20,\n",
    "        \"cot_output\": cot_prompt\n",
    "    }\n",
    "\n",
    "    for method_name, prompt in prompts.items():\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemma-3-27b-it\",\n",
    "                contents=prompt,\n",
    "                config=genai.types.GenerateContentConfig(\n",
    "                    temperature=0.0\n",
    "                )\n",
    "            )\n",
    "            time.sleep(6)\n",
    "\n",
    "            output_entry[method_name] = response.text.strip()\n",
    "            print(f\"[{method_name}] Finished index {i}\")\n",
    "            print(f\"{method_name}: {response.text.strip()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {i} with method {method_name}: {e}\")\n",
    "            output_entry[method_name] = \"ERROR\"\n",
    "\n",
    "    all_responses.append(output_entry)\n",
    "\n",
    "    # Save after each example (optional safety)\n",
    "    with open(\"gpt_method_2_predictions_on_test_set.json\", \"w\") as f:\n",
    "        json.dump(all_responses, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c33a227",
   "metadata": {},
   "source": [
    "# Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a09b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gpt_method_2_predictions_on_test_set.json\", \"r\") as f:\n",
    "    all_responses = json.load(f)\n",
    "all_responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2b87f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_triplets_df = pd.DataFrame(test_triplets)\n",
    "test_triplets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2787dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_prediction_results = pd.DataFrame(all_responses)\n",
    "triplet_prediction_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b622fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = test_triplets_df.merge(triplet_prediction_results)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a2134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "# Configuration\n",
    "INTERACTION_TYPES = {\"effect\", \"mechanism\", \"advice\", \"int\", \"no interaction\"}\n",
    "MODEL_MAP = {\n",
    "    'zero_shot_output': 'Zero-Shot',\n",
    "    'few_shot_output_2': 'Few-Shot-2',\n",
    "    'few_shot_output_5': 'Few-Shot-5',\n",
    "    'few_shot_output_10': 'Few-Shot-10',\n",
    "    'few_shot_output_20': 'Few-Shot-20',\n",
    "    'cot_output': 'CoT'\n",
    "}\n",
    "MODEL_ORDER = list(MODEL_MAP.keys())\n",
    "\n",
    "# --- Normalization ---\n",
    "def normalize_interaction_type(itype: str) -> str:\n",
    "    \"\"\"Map variations of interaction types to canonical form\"\"\"\n",
    "    itype = itype.strip().lower()\n",
    "    if itype in {\"advise\", \"adviced\", \"advised\", \"advice\"}:\n",
    "        return \"advice\"\n",
    "    return itype\n",
    "\n",
    "def normalize_drug_pair(drug1, drug2):\n",
    "    \"\"\"Create canonical drug pair representation\"\"\"\n",
    "    return tuple(sorted([drug1.strip().lower(), drug2.strip().lower()]))\n",
    "\n",
    "# --- Parsing ---\n",
    "def parse_prediction_output(pred_output):\n",
    "    \"\"\"Robust parser that requires exactly 2 drugs and 1 interaction type\"\"\"\n",
    "    if not isinstance(pred_output, str):\n",
    "        return []\n",
    "    \n",
    "    triplet_pattern = r'\\(([^,]+?),\\s*([^,]+?),\\s*([^)]+?)\\)'\n",
    "    raw_triplets = re.findall(triplet_pattern, pred_output)\n",
    "    \n",
    "    cleaned_triplets = []\n",
    "    for triplet in raw_triplets:\n",
    "        parts = [p.strip().lower() for p in triplet]\n",
    "        norm_parts = [normalize_interaction_type(p) for p in parts]\n",
    "        \n",
    "        # Identify interaction type candidates\n",
    "        interaction_candidates = [p for p in norm_parts if p in INTERACTION_TYPES]\n",
    "        drug_candidates = [parts[i] for i, p in enumerate(norm_parts) if p not in INTERACTION_TYPES]\n",
    "        \n",
    "        if len(interaction_candidates) == 1 and len(drug_candidates) == 2:\n",
    "            itype = interaction_candidates[0]\n",
    "            cleaned_triplets.append((drug_candidates[0], itype, drug_candidates[1]))\n",
    "            continue\n",
    "        \n",
    "        # Fallback: positional checks\n",
    "        if len(parts) == 3:\n",
    "            if norm_parts[1] in INTERACTION_TYPES:\n",
    "                cleaned_triplets.append((parts[0], norm_parts[1], parts[2]))\n",
    "            elif norm_parts[0] in INTERACTION_TYPES:\n",
    "                cleaned_triplets.append((parts[1], norm_parts[0], parts[2]))\n",
    "            elif norm_parts[2] in INTERACTION_TYPES:\n",
    "                cleaned_triplets.append((parts[0], norm_parts[2], parts[1]))\n",
    "    \n",
    "    return cleaned_triplets\n",
    "\n",
    "\n",
    "# --- Evaluation ---\n",
    "def evaluate_predictions(df):\n",
    "    \"\"\"Comprehensive evaluation with model comparison metrics\"\"\"\n",
    "    results = []\n",
    "    per_class_results = []\n",
    "    \n",
    "    for model_col in MODEL_ORDER:\n",
    "        if model_col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        model_name = MODEL_MAP[model_col]\n",
    "        \n",
    "        # Initialize metrics\n",
    "        metrics = {\n",
    "            'Model': model_name,\n",
    "            'Accuracy': 0,\n",
    "            'Micro_Precision': 0, 'Micro_Recall': 0, 'Micro_F1': 0,\n",
    "            'Macro_Precision': 0, 'Macro_Recall': 0, 'Macro_F1': 0,\n",
    "            'Weighted_Precision': 0, 'Weighted_Recall': 0, 'Weighted_F1': 0,\n",
    "        }\n",
    "        \n",
    "        class_metrics = {itype: {'TP': 0, 'FP': 0, 'FN': 0, 'TN': 0} for itype in INTERACTION_TYPES}\n",
    "        all_true = []\n",
    "        all_pred = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Process ground truth\n",
    "            gold_triplets = set()\n",
    "            for triplet in row['triplets']:\n",
    "                if len(triplet) == 3:\n",
    "                    d1, itype, d2 = [x.strip().lower() for x in triplet]\n",
    "                    itype = normalize_interaction_type(itype)\n",
    "                    gold_triplets.add((normalize_drug_pair(d1, d2), itype))\n",
    "            \n",
    "            # Process predictions\n",
    "            pred_triplets = set()\n",
    "            for d1, itype, d2 in parse_prediction_output(row[model_col]):\n",
    "                itype = normalize_interaction_type(itype)\n",
    "                pred_triplets.add((normalize_drug_pair(d1, d2), itype))\n",
    "            \n",
    "            # Create lists for sklearn metrics\n",
    "            for itype in INTERACTION_TYPES:\n",
    "                # True positive: both gold and prediction have this interaction\n",
    "                if any((dp, t) in gold_triplets and t == itype for dp, t in pred_triplets):\n",
    "                    class_metrics[itype]['TP'] += 1\n",
    "                    all_true.append(itype)\n",
    "                    all_pred.append(itype)\n",
    "                # False positive: prediction has this interaction but gold doesn't\n",
    "                elif any(t == itype for dp, t in pred_triplets):\n",
    "                    class_metrics[itype]['FP'] += 1\n",
    "                    all_pred.append(itype)\n",
    "                    # Assign the true label\n",
    "                    true_label = \"no interaction\"\n",
    "                    for dp, t in gold_triplets:\n",
    "                        if dp in [dp_p for dp_p, t_p in pred_triplets if t_p == itype]:\n",
    "                            true_label = t\n",
    "                            break\n",
    "                    all_true.append(true_label)\n",
    "                # False negative: gold has this interaction but prediction doesn't\n",
    "                elif any(t == itype for dp, t in gold_triplets):\n",
    "                    class_metrics[itype]['FN'] += 1\n",
    "                    all_true.append(itype)\n",
    "                    all_pred.append(\"no interaction\")  # Missed prediction\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct = sum(1 for t, p in zip(all_true, all_pred) if t == p)\n",
    "        total = len(all_true) if all_true else 1\n",
    "        metrics['Accuracy'] = correct / total if total > 0 else 0\n",
    "        \n",
    "        # Calculate micro, macro, and weighted metrics using sklearn\n",
    "        if all_true and all_pred:\n",
    "            micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(\n",
    "                all_true, all_pred, average='micro', zero_division=0)\n",
    "            macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
    "                all_true, all_pred, average='macro', zero_division=0)\n",
    "            weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(\n",
    "                all_true, all_pred, average='weighted', zero_division=0)\n",
    "            \n",
    "            metrics.update({\n",
    "                'Micro_Precision': micro_precision,\n",
    "                'Micro_Recall': micro_recall,\n",
    "                'Micro_F1': micro_f1,\n",
    "                'Macro_Precision': macro_precision,\n",
    "                'Macro_Recall': macro_recall,\n",
    "                'Macro_F1': macro_f1,\n",
    "                'Weighted_Precision': weighted_precision,\n",
    "                'Weighted_Recall': weighted_recall,\n",
    "                'Weighted_F1': weighted_f1\n",
    "            })\n",
    "        \n",
    "        results.append(metrics)\n",
    "        \n",
    "        # Calculate per-class metrics\n",
    "        for itype in INTERACTION_TYPES:\n",
    "            cm = class_metrics[itype]\n",
    "            p = cm['TP'] / (cm['TP'] + cm['FP']) if cm['TP'] + cm['FP'] > 0 else 0\n",
    "            r = cm['TP'] / (cm['TP'] + cm['FN']) if cm['TP'] + cm['FN'] > 0 else 0\n",
    "            f = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "            support = cm['TP'] + cm['FN']\n",
    "            \n",
    "            per_class_results.append({\n",
    "                'Model': model_name,\n",
    "                'Interaction': itype,\n",
    "                'Precision': p,\n",
    "                'Recall': r,\n",
    "                'F1': f,\n",
    "                'Support': support\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results), pd.DataFrame(per_class_results)\n",
    "\n",
    "# --- Report ---\n",
    "def generate_comparison_report(global_df, class_df):\n",
    "    \"\"\"Create model comparison report with sorting\"\"\"\n",
    "    global_df = global_df.sort_values('Micro_F1', ascending=False)\n",
    "    \n",
    "    # Format numbers\n",
    "    global_df = global_df.round(4)\n",
    "    class_df = class_df.round(4)\n",
    "    \n",
    "    report = \"MODEL COMPARISON REPORT\\n\"\n",
    "    report += \"=======================\\n\\n\"\n",
    "    report += \"Global Metrics:\\n\"\n",
    "    report += global_df.to_string(index=False) + \"\\n\\n\"\n",
    "    report += \"Per-Class Metrics:\\n\"\n",
    "    report += class_df.to_string(index=False)\n",
    "    \n",
    "    best_model = global_df.iloc[0]\n",
    "    report += f\"\\n\\nBest Model: {best_model['Model']} (Macro F1={best_model['Macro_F1']:.4f}, Micro F1={best_model['Micro_F1']:.4f})\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    global_results, class_results = evaluate_predictions(merged_df)\n",
    "    report = generate_comparison_report(global_results, class_results)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd04cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_class_distributions(class_df):\n",
    "    \"\"\"\n",
    "    Visualize correct prediction distribution per class across models.\n",
    "    Each bar shows recall (correct / total).\n",
    "    \"\"\"\n",
    "    classes = sorted(class_df['Interaction'].unique())\n",
    "    models = class_df['Model'].unique()\n",
    "\n",
    "    # Make one subplot per interaction type\n",
    "    fig, axes = plt.subplots(len(classes), 1, figsize=(10, 4 * len(classes)), sharex=True)\n",
    "\n",
    "    if len(classes) == 1:\n",
    "        axes = [axes]  # Ensure iterable\n",
    "\n",
    "    for ax, itype in zip(axes, classes):\n",
    "        subset = class_df[class_df['Interaction'] == itype]\n",
    "\n",
    "        recalls = subset.set_index('Model')['Recall']\n",
    "        supports = subset.set_index('Model')['Support']\n",
    "\n",
    "        # Plot recall bars\n",
    "        recalls.plot(kind='bar', ax=ax, color='skyblue', edgecolor='black')\n",
    "\n",
    "        # Annotate bars with \"TP / Support\"\n",
    "        for i, model in enumerate(recalls.index):\n",
    "            tp = int(round(recalls[model] * supports[model]))\n",
    "            ax.text(i, recalls[model] + 0.02, f\"{tp}/{supports[model]}\", \n",
    "                    ha='center', fontsize=9, rotation=0)\n",
    "\n",
    "        ax.set_ylim(0, 1.05)\n",
    "        ax.set_title(f\"Class: {itype}\", fontsize=14)\n",
    "        ax.set_ylabel(\"Recall (Correct / Total)\")\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    plt.suptitle(\"Per-Class Prediction Distribution per Model\", fontsize=16, y=1.02)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb0a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    global_results, class_results = evaluate_predictions(merged_df)\n",
    "    report = generate_comparison_report(global_results, class_results)\n",
    "    print(report)\n",
    "\n",
    "    # 👇 Add this line to display the plots\n",
    "    plot_class_distributions(class_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5c646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define valid interaction types\n",
    "INTERACTION_TYPES = {\"effect\", \"mechanism\", \"advice\", \"int\", \"no interaction\"}\n",
    "\n",
    "def process_triplets(df):\n",
    "    # Step 1: Explode ground truth triplets\n",
    "    exploded_rows = []\n",
    "    for _, row in merged_df.iterrows():\n",
    "        for triplet in row['triplets']:\n",
    "            exploded_rows.append({\n",
    "                'index': row['index'],\n",
    "                'sentence': row['sentence'],\n",
    "                'ground_truth_triplet': triplet,\n",
    "                'ground_truth_type': triplet[1],  # interaction_type from triplet\n",
    "                'zero_shot_output': row['zero_shot_output'],\n",
    "                'few_shot_output_2': row['few_shot_output_2'],\n",
    "                'few_shot_output_5': row['few_shot_output_5'],\n",
    "                'few_shot_output_10': row['few_shot_output_10'],\n",
    "                'few_shot_output_20': row['few_shot_output_20'],\n",
    "                'cot_output': row['cot_output'],\n",
    "            })\n",
    "    df_expanded = pd.DataFrame(exploded_rows)\n",
    "    return df_expanded\n",
    "\n",
    "def parse_prediction_output(pred_output):\n",
    "    \"\"\"Parse prediction output into cleaned triplets with consistent format\"\"\"\n",
    "    if not isinstance(pred_output, str):\n",
    "        return []\n",
    "    \n",
    "    # Find all triplets in the string\n",
    "    triplet_pattern = r'\\(([^,]+?),\\s*([^,]+?),\\s*([^)]+?)\\)'\n",
    "    raw_triplets = re.findall(triplet_pattern, pred_output)\n",
    "    \n",
    "    cleaned_triplets = []\n",
    "    for triplet in raw_triplets:\n",
    "        parts = [p.strip() for p in triplet]\n",
    "        \n",
    "        # Case 1: Middle element is interaction type\n",
    "        if parts[1].lower() in INTERACTION_TYPES:\n",
    "            cleaned_triplets.append((parts[0], parts[1].lower(), parts[2]))\n",
    "        \n",
    "        # Case 2: Last element is interaction type\n",
    "        elif parts[2].lower() in INTERACTION_TYPES:\n",
    "            cleaned_triplets.append((parts[0], parts[2].lower(), parts[1]))\n",
    "        \n",
    "        # Case 3: Try to identify interaction by position\n",
    "        else:\n",
    "            # Try first element as interaction\n",
    "            if parts[0].lower() in INTERACTION_TYPES:\n",
    "                cleaned_triplets.append((parts[1], parts[0].lower(), parts[2]))\n",
    "            # Try last element as interaction (even if not in set)\n",
    "            elif parts[2].lower() not in INTERACTION_TYPES and parts[1].lower() not in INTERACTION_TYPES:\n",
    "                # Final fallback: assume last element is interaction\n",
    "                cleaned_triplets.append((parts[0], parts[2].lower(), parts[1]))\n",
    "    \n",
    "    return cleaned_triplets\n",
    "\n",
    "def match_predictions(df_expanded, column_name):\n",
    "    \"\"\"Match predictions to ground truth with per-type tracking\"\"\"\n",
    "    matched_data = []\n",
    "    \n",
    "    for _, row in df_expanded.iterrows():\n",
    "        # Get ground truth triplet\n",
    "        gt_drug1, gt_interaction, gt_drug2 = row['ground_truth_triplet']\n",
    "        gt_interaction = gt_interaction.lower()\n",
    "        gt_drug_pair = tuple(sorted([gt_drug1.strip(), gt_drug2.strip()]))\n",
    "        \n",
    "        # Parse predictions\n",
    "        predictions = parse_prediction_output(row[column_name])\n",
    "        pred_matches = {tuple(sorted([d1.strip(), d2.strip()])): itype \n",
    "                        for d1, itype, d2 in predictions}\n",
    "        \n",
    "        # Check match for this specific triplet\n",
    "        match_found = False\n",
    "        if gt_drug_pair in pred_matches:\n",
    "            match_found = (pred_matches[gt_drug_pair] == gt_interaction)\n",
    "        \n",
    "        matched_data.append({\n",
    "            'index': row['index'],\n",
    "            'sentence': row['sentence'],\n",
    "            'ground_truth_triplet': (gt_drug1, gt_interaction, gt_drug2),\n",
    "            'ground_truth_type': gt_interaction,\n",
    "            'predicted_triplets': predictions,\n",
    "            'match': match_found,\n",
    "            'drug_pair': gt_drug_pair\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(matched_data)\n",
    "\n",
    "# Generate matched dataframes\n",
    "df_expanded = process_triplets(merged_df)  # Ensure this creates one row per triplet\n",
    "\n",
    "matched_dfs = {\n",
    "    'zero_shot': match_predictions(df_expanded, 'zero_shot_output'),\n",
    "    'few_shot_2': match_predictions(df_expanded, 'few_shot_output_2'),\n",
    "    'few_shot_5': match_predictions(df_expanded, 'few_shot_output_5'),\n",
    "    'few_shot_10': match_predictions(df_expanded, 'few_shot_output_10'),\n",
    "    'few_shot_20': match_predictions(df_expanded, 'few_shot_output_20'),\n",
    "    'cot': match_predictions(df_expanded, 'cot_output')\n",
    "}\n",
    "\n",
    "# Calculate per-type accuracy\n",
    "results = []\n",
    "\n",
    "for method_name, df in matched_dfs.items():\n",
    "    # Overall accuracy\n",
    "    overall_acc = df['match'].mean()\n",
    "    \n",
    "    # Per-type accuracy\n",
    "    type_acc = {}\n",
    "    for itype in INTERACTION_TYPES:\n",
    "        type_df = df[df['ground_truth_type'] == itype]\n",
    "        if len(type_df) > 0:\n",
    "            type_acc[itype] = type_df['match'].mean()\n",
    "        else:\n",
    "            type_acc[itype] = None  # Handle missing types\n",
    "    \n",
    "    results.append({\n",
    "        'method': method_name,\n",
    "        'overall_accuracy': overall_acc,\n",
    "        **{f'{itype}_accuracy': acc for itype, acc in type_acc.items()}\n",
    "    })\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808e4ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pair_and_interaction_counts(merged_df):\n",
    "    TECHNIQUE_ORDER = [\n",
    "        'zero_shot_output',\n",
    "        'few_shot_output_2',\n",
    "        'few_shot_output_5',\n",
    "        'few_shot_output_10',\n",
    "        'few_shot_output_20',\n",
    "        'cot_output'\n",
    "    ]\n",
    "\n",
    "    TECHNIQUE_MAP = {\n",
    "        'zero_shot_output': 'Zero-Shot',\n",
    "        'few_shot_output_2': 'Few-Shot (2)',\n",
    "        'few_shot_output_5': 'Few-Shot (5)',\n",
    "        'few_shot_output_10': 'Few-Shot (10)',\n",
    "        'few_shot_output_20': 'Few-Shot (20)',\n",
    "        'cot_output': 'Chain-of-Thought'\n",
    "    }\n",
    "    \n",
    "    per_method_details = []\n",
    "    per_interaction_details = []\n",
    "    \n",
    "    for tech_col in TECHNIQUE_ORDER:\n",
    "        if tech_col not in merged_df.columns:\n",
    "            continue\n",
    "        \n",
    "        tech_name = TECHNIQUE_MAP.get(tech_col, tech_col)\n",
    "        \n",
    "        interaction_correct, interaction_total = 0, 0\n",
    "        per_class = {itype: {'tp': 0, 'fn': 0} for itype in INTERACTION_TYPES}\n",
    "        \n",
    "        for _, row in merged_df.iterrows():\n",
    "            # Ground truth triplets\n",
    "            gold_triplets = set()\n",
    "            if isinstance(row['triplets'], list):\n",
    "                for triplet in row['triplets']:\n",
    "                    if len(triplet) == 3:\n",
    "                        d1, itype, d2 = [x.strip().lower() for x in triplet]\n",
    "                        itype = normalize_interaction_type(itype)  # <-- FIX\n",
    "                        gold_triplets.add((normalize_drug_pair(d1, d2), itype))\n",
    "            \n",
    "            # Predictions\n",
    "            pred_triplets = set()\n",
    "            for d1, itype, d2 in parse_prediction_output(row[tech_col]):\n",
    "                itype = normalize_interaction_type(itype)  # <-- FIX\n",
    "                pred_triplets.add((normalize_drug_pair(d1, d2), itype))\n",
    "            \n",
    "            # Pair-level counts (correct if both pair+interaction match)\n",
    "            gold_pairs = {dp for dp, _ in gold_triplets}\n",
    "            for dp, it in pred_triplets:\n",
    "                if dp in gold_pairs:  # model predicted some interaction for a true pair\n",
    "                    interaction_total += 1\n",
    "                    if (dp, it) in gold_triplets:\n",
    "                        interaction_correct += 1\n",
    "            \n",
    "            # Per-interaction type counts\n",
    "            for itype in INTERACTION_TYPES:\n",
    "                gold_sub = {(dp, it) for dp, it in gold_triplets if it == itype}\n",
    "                pred_sub = {(dp, it) for dp, it in pred_triplets if it == itype}\n",
    "                \n",
    "                tp = len(gold_sub & pred_sub)\n",
    "                fn = len(gold_sub - pred_sub)\n",
    "                \n",
    "                per_class[itype]['tp'] += tp\n",
    "                per_class[itype]['fn'] += fn\n",
    "        \n",
    "        # Store per-method totals\n",
    "        per_method_details.append({\n",
    "            \"Technique\": tech_name,\n",
    "            \"Correct Pairs\": interaction_correct,\n",
    "            \"Total Pairs\": interaction_total\n",
    "        })\n",
    "        \n",
    "        # Store per-interaction type totals\n",
    "        for itype in INTERACTION_TYPES:\n",
    "            tp = per_class[itype]['tp']\n",
    "            fn = per_class[itype]['fn']\n",
    "            per_interaction_details.append({\n",
    "                \"Technique\": tech_name,\n",
    "                \"Interaction Type\": itype,\n",
    "                \"Correct\": tp,\n",
    "                \"Total\": tp + fn\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(per_method_details), pd.DataFrame(per_interaction_details)\n",
    "method_df, interaction_df = calculate_pair_and_interaction_counts(merged_df)\n",
    "print(\"PER-METHOD PAIR COUNTS:\")\n",
    "print(method_df.to_string(index=False))\n",
    "print(\"\\nPER-INTERACTION TYPE COUNTS:\")\n",
    "print(interaction_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfadd63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46f828a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d93adf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
